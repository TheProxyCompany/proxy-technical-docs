docs/index.md
---
# The Proxy Company Documentation

Welcome to the technical documentation for The Proxy Company.

This documentation will help you understand and utilize our technologies effectively.

## Our Technologies

<div class="grid cards" markdown>

-   :material-engine-outline: __Proxy Structuring Engine (PSE)__

    ---
    Dynamically structure language models to produce outputs that adhere to specific requirements without sacrificing their creative capabilities.

    [:octicons-book-24: PSE Documentation](/pse/){: .md-button .md-button--primary }

-   :material-robot-outline: __Proxy Base Agent (PBA)__

    ---
    A foundational agent built on top of the Proxy Structuring Engine. We share it as a research preview and invite developers to experiment with it.

    [:octicons-book-24: PBA Documentation](/pba/){: .md-button .md-button--primary }

</div>

## Additional Resources

- [Company Website](https://theproxycompany.com)
- [GitHub Organization](https://github.com/TheProxyCompany)


---
pba-docs/docs/index.md
---
# Proxy Base Agent

The **Proxy Base Agent (PBA)** is a foundation agent built with the **Proxy Structuring Engine (PSE)**, which provides the underlying framework for managing the agent's state, controlling the flow of execution, and interacting with the language model.

The base agent is designed to **rapidly prototype and develop LLM-powered agents** with a focus on **local execution, stateful interactions, and extensibility**.

The **PSE** augments **language models** at runtime, allowing them to function effectively as agents - capable of adhering to predefined workflows, multi-step reasoning, and external tool usage.

## What is an Agent?

> An agent is a system that takes actions in an environment.

## Proxy Base Agent

The Proxy Base Agent operates through a structured workflow defined by a **state graph**, transitioning through clearly defined **planning** and **action** phases:

```mermaid
flowchart TD
    Start([Start]) --> Plan
    Start -. force_planning = false .-> Action

    subgraph Plan["Planning Phase"]
        PlanningChoice{"Choose planning type"}
        Thinking["Thinking"]
        Scratchpad["Scratchpad"]
        InnerMonologue["Inner Monologue"]

        PlanningChoice --> Thinking
        PlanningChoice --> Scratchpad
        PlanningChoice --> InnerMonologue
    end

    subgraph Action["Action Phase"]
        ActionChoice{"Choose action type"}
        ToolAction["Tool Call"]
        CodeAction["Python Code"]

        ActionChoice -- "Tool" --> ToolAction
        ActionChoice -- "Code" --> CodeAction
    end

    Plan --> PlanLoop{"More planning needed?"}
    PlanLoop -- "Yes" --> Plan
    PlanLoop -- "No" --> Action

    Action --> Finish([Finish])

    classDef phase fill:#DAD0AF,stroke:#0c5460,border-color:#024645
    classDef decision fill:#024645,stroke:#DAD0AF,color:#DAD0AF,border-color:#DAD0AF,shape:diamond
    classDef state fill:#024645,stroke:#DAD0AF,color:#DAD0AF,border-color:#DAD0AF
    classDef terminal fill:#024645,stroke:#DAD0AF,color:#DAD0AF,border-color:#DAD0AF,shape:stadium

    class Plan,Action phase
    class PlanLoop,ActionChoice,StepCheck decision
    class PlanningChoice,Thinking,Scratchpad,InnerMonologue state
    class ToolAction,CodeAction state
    class Start,Finish terminal

    linkStyle default stroke:#024645
```

### Planning Phase

The agent first enters a planning loop, choosing between internal states to reason about the task:

*   **Thinking**: Deliberate analysis and planning.
*   **Scratchpad**: Quick notes and working memory.
*   **Inner Monologue**: Detailed self-reflection and narrative reasoning.

### Action Phase

Once planning is complete, the agent selects an action to interact with the environment:

*   **Tool Calls**: Invokes external tools or APIs via guaranteed schemas.
*   **Python Code Execution**: (Optional) Runs Python code snippets.

### State Graph

This state graph describes the base behavior of the agent.
It can be extended and modified to support more complex agentic behaviors.

## Key Capabilities

PBA leverages PSE to deliver capabilities beyond conventional agent frameworks:

*   ðŸ§  **True Stateful Execution:** Define and enforce complex workflows using an explicit HSM (Plan âž” Act). PSE guarantees state consistency.
*   âœ… **100% Reliable Tool Use:** Eliminate runtime errors from malformed API calls or hallucinated arguments via schema validation *during generation*.
*   âš¡ **Dynamic Runtime Adaptation (MCP):** Connect to external MCP servers to integrate new tools on-the-fly. PBA dynamically updates its configuration, allowing immediate, reliable use of new capabilities.
*   âš™ï¸ **Predictable Control Flow:** Explicitly define agent reasoning patterns and action sequences for deterministic behavior.
*   ðŸ”Œ **Universal LLM Compatibility:** Designed for local models using various backends (MLX, PyTorch supported).
*   ðŸ§© **Modular & Extensible:** Build bespoke agents by adding custom tools, states, or modifying the core HSM architecture.

## Installation & Quickstart

Prerequisites:

- Python 3.10 or higher
- Linux, macOS, or Windows
- Hardware requirements vary depending on the underlying language model you are using.

Get the Proxy Base Agent running quickly:

```bash
# Install required dependencies
pip install proxy-base-agent

# Launch interactive setup wizard
python -m agent
```

# More Information

For more detailed guides, see:

- [Installation Guide](getting-started/installation.md)
- [Quickstart Tutorial](getting-started/quickstart.md)

- [Core Concepts](concepts/index.md)
- [Extending the Agent](extending/index.md)
- [Frontends](frontends/index.md)

---

[View on GitHub](https://github.com/TheProxyCompany/proxy-base-agent){: .md-button .md-button--primary }


---
pba-docs/docs/frontends/index.md
---
# LLM Frontends

The Proxy Base Agent (PBA) is designed to work with different local Large Language Model (LLM) inference backends. This flexibility is achieved through the **Frontend** abstraction layer.

## The Frontend Abstraction

The `agent.llm.frontend.Frontend` class defines an abstract interface that decouples the core agent logic from the specifics of how LLM inference is performed. Any backend wanting to integrate with PBA needs to implement this interface.

Key responsibilities of a Frontend implementation include:

*   Loading the specified LLM and its associated tokenizer.
*   Providing a standardized `inference()` method that takes a tokenized prompt and yields generated token IDs one by one (or in small chunks).
*   Integrating with PBA's `StructuringEngine` by accepting it as an argument to `inference()` and applying its `process_logits` and `sample` methods within the generation loop.
*   Optionally supporting KV caching for performance optimization (`supports_reusing_prompt_cache()`, `save_cache_to_file()`, `load_cache_from_file()`).

## Supported Frontends

PBA currently includes built-in support for the following frontends:

*   **[MLX](./mlx.md):** Optimized for Apple Silicon (M-series chips), leveraging the MLX framework for efficient local inference.
*   **[PyTorch](./pytorch.md):** Supports running models using PyTorch on CPUs or GPUs (NVIDIA/AMD).

## Choosing a Frontend

The appropriate frontend is typically selected during the interactive setup wizard (`python -m agent`) based on your hardware and the format of the local model you choose.

*   If you are on an Apple Silicon Mac, **MLX** is generally recommended for best performance.
*   If you are on Linux/Windows or have a compatible GPU, **PyTorch** is a common choice.

The `agent.llm.local.LocalInference` class manages the selected `Frontend` instance for the `Agent`.

---
pba-docs/docs/frontends/mlx.md
---
# MLX Frontend

The MLX frontend allows the Proxy Base Agent (PBA) to run inference using Large Language Models optimized for Apple Silicon (M-series chips) via the [MLX framework](https://github.com/ml-explore/mlx).

## Overview

MLX is a NumPy-like array framework designed by Apple for efficient machine learning on Apple Silicon. The `agent.llm.frontend.mlx.MLXInference` class implements the `Frontend` interface for MLX models.

**Key Features:**

*   **Optimized Performance:** Leverages the unified memory architecture and Neural Engine (ANE) of Apple Silicon for fast local inference.
*   **Model Compatibility:** Works with models converted to the MLX format (often available on Hugging Face hubs like `mlx-community`).
*   **KV Caching:** Supports efficient Key-Value caching, including saving/loading system prompt caches to disk for faster startup (`supports_reusing_prompt_cache()` returns `True`).
*   **PSE Integration:** Seamlessly integrates with the `StructuringEngine` for constrained generation during the `generate_step` process.

## Usage

1.  **Installation:** Ensure you have the necessary MLX dependencies installed. This is typically handled by installing PBA with the `[mlx]` extra:
    ```bash
    pip install proxy-base-agent[mlx]
    # or
    uv pip install proxy-base-agent[mlx]
    ```
    You also need the `mlx-lm` package:
    ```bash
    pip install mlx-lm
    # or
    uv pip install mlx-lm
    ```

2.  **Model Selection:** During the PBA setup wizard (`python -m agent`), choose a model compatible with MLX (e.g., from the `mlx-community` hub on Hugging Face or one you have converted locally). Select "MLX" as the inference backend when prompted.

3.  **Configuration:** The `LocalInference` class will automatically instantiate `MLXInference` when an MLX model path and the MLX frontend are selected. Relevant `inference_kwargs` (like `temp`, `seed`, `max_tokens`, caching options) passed to the `Agent` constructor will be used during generation.

## How it Works

*   **Loading:** `MLXInference` uses `mlx_proxy.utils.load_model` to load the model and `agent.llm.tokenizer.Tokenizer.load` for the tokenizer.
*   **Inference Loop:** The `inference()` method uses `mlx_proxy.generate_step.generate_step`, passing the `engine.process_logits` function to the `logits_processors` argument and a sampler created via `engine.sample` wrapping `mlx_proxy.samplers.make_sampler`.
*   **Caching:** It utilizes `mlx_proxy.cache.BaseCache` for KV caching and implements the `save_cache_to_file` and `load_cache_from_file` methods using `safetensors` for persistent prompt caching.

The MLX frontend provides an efficient way to run PBA locally on Apple Silicon hardware.

---
pba-docs/docs/frontends/pytorch.md
---
# PyTorch Frontend

The PyTorch frontend enables the Proxy Base Agent (PBA) to utilize Large Language Models loaded via the popular [PyTorch](https://pytorch.org/) framework and Hugging Face `transformers`.

## Overview

This frontend uses the `agent.llm.frontend.torch.TorchInference` class, which integrates with standard PyTorch models (like `LlamaForCausalLM` or other `transformers` models) and the PSE `StructuringEngine`.

**Key Features:**

*   **Broad Compatibility:** Works with a wide range of Hugging Face `transformers` models compatible with PyTorch.
*   **Hardware Flexibility:** Runs on CPUs or GPUs (NVIDIA, AMD) supported by PyTorch.
*   **PSE Integration:** Uses the `PSETorchMixin` from the `pse` library to easily integrate the `StructuringEngine`'s `process_logits` and `sample` methods into the standard `transformers` `generate()` workflow.

## Usage

1.  **Installation:** Ensure you have PyTorch and the necessary `transformers` dependencies installed. This is typically handled by installing PBA with the `[torch]` extra:
    ```bash
    pip install proxy-base-agent[torch]
    # or
    uv pip install proxy-base-agent[torch]
    ```
    You may need to install a specific version of PyTorch separately depending on your hardware (CPU/CUDA/ROCm). See the [PyTorch installation guide](https://pytorch.org/get-started/locally/).

2.  **Model Selection:** During the PBA setup wizard (`python -m agent`), choose a model compatible with PyTorch (most standard Hugging Face models). Select "PyTorch" as the inference backend when prompted.

3.  **Configuration:** The `LocalInference` class will automatically instantiate `TorchInference` when a PyTorch-compatible model path and the PyTorch frontend are selected. Relevant `inference_kwargs` (like `temp`, `seed`, `max_tokens`, `top_k`, `top_p`) passed to the `Agent` constructor will be used by the `model.generate()` method.

## How it Works

*   **Loading:** `TorchInference` loads the model using `transformers.AutoModelForCausalLM.from_pretrained` (specifically via the `PSE_Torch` class which incorporates the `PSETorchMixin`) and the tokenizer using `agent.llm.tokenizer.Tokenizer.load`.
*   **Mixin Integration:** The `PSETorchMixin` modifies the model's `_sample` method (used by `generate` when `do_sample=True`) to:
    *   Include `engine.process_logits` in the `logits_processor` list.
    *   Use `engine.sample` (wrapping a basic multinomial sampler or argmax) for token selection.
*   **Inference Loop:** The `inference()` method sets up a `TextIteratorStreamer` and runs `model.generate()` in a separate thread, yielding tokens as they become available from the streamer.
*   **Caching:** Currently, the PyTorch frontend in PBA **does not** implement persistent KV cache saving/loading to disk like the MLX frontend (`supports_reusing_prompt_cache()` returns `False`). Standard `transformers` in-memory KV caching during generation *is* used if enabled (`use_cache=True`).

The PyTorch frontend offers broad model compatibility for running PBA on various hardware configurations.

---
pba-docs/docs/getting-started/installation.md
---
# Installation

This guide covers installing the Proxy Base Agent (PBA) and its dependencies.

## Prerequisites

*   **Python:** Version 3.10 or higher.
*   **Operating System:** Linux, macOS, or Windows.
*   **LLM Backend:** You need a compatible local LLM setup. PBA currently supports:
    *   [MLX](https://github.com/ml-explore/mlx) (for Apple Silicon Macs)
    *   [PyTorch](https://pytorch.org/) (CPU or GPU)
    *(Support for other backends may be added in the future).*
*   **Hardware:** Requirements depend heavily on the LLM you choose to run locally. Ensure your system meets the minimum requirements for your selected model.

## Installation Methods

Choose the method that best suits your needs:

### Method 1: Using `pip` (Recommended)

This is the simplest way to install the latest stable release of PBA from PyPI.

```bash
# Install the core PBA package
pip install proxy-base-agent

# --- Framework Extras ---
# Install extras for the LLM backend you intend to use:

# For MLX (Apple Silicon):
pip install proxy-base-agent[mlx]

# For PyTorch:
pip install proxy-base-agent[torch]
# (Ensure you have a compatible PyTorch version installed separately if needed)
```

### Method 2: Using `uv` (Fast Alternative)

If you use `uv`, Astral's fast package manager:

```bash
# Install the core PBA package
uv pip install proxy-base-agent

# --- Framework Extras ---
# For MLX (Apple Silicon):
uv pip install proxy-base-agent[mlx]

# For PyTorch:
uv pip install proxy-base-agent[torch]
```

### Method 3: From Source (Development / Latest Features)

Install directly from the GitHub repository for development or to get the absolute latest code (potentially unstable).

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/TheProxyCompany/proxy-base-agent.git
    cd proxy-base-agent
    ```

2.  **Install in Editable Mode:**
    We recommend using a virtual environment.

    *   **Using `pip`:**
        ```bash
        # Install core package and common dev dependencies
        pip install -e ".[dev]"

        # Install specific framework extras if needed
        pip install -e ".[mlx]"
        # or
        pip install -e ".[torch]"
        ```
    *   **Using `uv`:**
        ```bash
        # Install core package and common dev dependencies
        uv pip install -e ".[dev]"

        # Install specific framework extras if needed
        uv pip install -e ".[mlx]"
        # or
        uv pip install -e ".[torch]"
        ```
    The `-e` flag installs the package in "editable" mode, meaning changes you make to the source code will be reflected immediately when you run the agent.

## Verifying Installation

After installation, you should be able to run the agent's setup wizard:

```bash
python -m agent
```

If the wizard starts without import errors, the installation was successful.

## Next Steps

*   Follow the [Quickstart Guide](./quickstart.md) to run the agent and interact with it.
*   Explore the [Core Concepts](../concepts/index.md) to understand how PBA works.

If you encounter issues, please open an issue on [GitHub](https://github.com/TheProxyCompany/proxy-base-agent/issues).

---
pba-docs/docs/getting-started/quickstart.md
---
# Quickstart

This guide provides the fastest way to get the Proxy Base Agent (PBA) running locally and see it in action.

We'll launch the agent using its built-in command-line interface (CLI) and interactive setup wizard.

## Prerequisites

*   Ensure you have met the requirements and installed PBA by following the [Installation Guide](./installation.md).

## 1. Launch the Agent

Open your terminal or command prompt, navigate to the directory where you installed or cloned the `proxy-base-agent`, and run the following command:

```bash
python -m agent
```

This command executes the main entry point of the PBA package.

## 2. Interactive Setup Wizard

Upon launching, PBA will start an interactive setup wizard directly in your terminal. This wizard guides you through configuring the agent for the current session:

*   **Agent Identity:** You'll be prompted to provide a name for the agent instance.
*   **System Prompt:** Choose a base system prompt template that defines the agent's core personality and instructions.
*   **Language Model (LLM):** Select the local LLM you want the agent to use. PBA will scan common locations for compatible models (MLX or PyTorch, depending on your setup). You can also specify a Hugging Face model ID to download.
*   **Inference Backend:** Choose the backend (MLX or PyTorch) if multiple are available for your selected model.
*   **Configuration Mode:**
    *   **Basic:** Uses sensible defaults for most parameters (recommended for first use).
    *   **Advanced:** Allows detailed configuration of capabilities (Python execution, voice), planning behavior, performance options, inference parameters, and more.

Follow the prompts, selecting the options appropriate for your setup. For your first run, using "Basic" configuration mode is recommended.

## 3. Interact

Once the setup wizard completes and the model is loaded, the agent will be ready. You'll see a prompt like:

```
> Enter your message [enter to send, Ctrl+C to exit]:
```

Type your message or instruction to the agent and press Enter.

Observe the agent's output in the terminal. If you enabled advanced options or logging, you might see:

*   **Planning States:** Output delimited by tags like `[thinking]...[/thinking]` or `[scratchpad]...[/scratchpad]` showing the agent's internal reasoning (if the chosen prompt uses them).
*   **Tool Calls:** Structured JSON output within `[tool_call]...[/tool_call]` tags when the agent decides to use a tool.
*   **Tool Results:** Output from the tool execution, often prefixed with `[Tool]`.
*   **Final Response:** The agent's message to you, typically generated via the `send_message` tool.

Experiment with different prompts to see how the agent plans, uses tools (if available), and responds.

## Next Steps

*   Explore the [Core Concepts](../concepts/index.md) to understand the underlying architecture.
*   Learn how to add [Custom Tools](../extending/custom-tools.md).
*   Configure different [LLM Frontends](../frontends/index.md).


---
pba-docs/docs/concepts/index.md
---
# Core Concepts

The Proxy Base Agent (PBA) is built upon several core concepts that enable its reliable and extensible nature. Understanding these concepts is key to effectively using and customizing the agent.

## 1. State Machine Architecture

PBA's behavior is not driven by simple prompt chains, but by a formal **Hierarchical State Machine (HSM)**. This machine defines distinct phases (like Planning and Action) and specific states (like Thinking, Tool Call) the agent transitions through.

*   **Reliability:** The HSM structure, enforced by the underlying Proxy Structuring Engine (PSE), guarantees that the agent follows the defined workflow predictably.
*   **Control:** Developers have explicit control over the agent's execution flow.

[Learn more about the State Machine](./state-machine.md)

## 2. Agent States

Each step in the agent's HSM is represented by an **Agent State**. These states encapsulate specific functionalities:

*   **Planning States:** `Thinking`, `Scratchpad`, `InnerMonologue` allow the agent to reason and plan internally.
*   **Action States:** `ToolCallState`, `Python` enable the agent to interact with the external environment or execute code.

Each state uses its own nested PSE `StateMachine` to define and enforce the structure of the content generated *within* that state.

[Learn more about States](./states.md)

## 3. Tools

Tools are external capabilities the agent can invoke during its Action Phase. They allow the agent to interact with APIs, databases, code interpreters, or perform specialized tasks.

*   **Reliable Invocation:** PBA uses PSE to guarantee that the arguments provided to a tool call match the tool's defined schema *before* execution.
*   **Extensibility:** New tools can be easily added to expand the agent's capabilities.

[Learn more about Tools](./tools.md)

## 4. Proxy Structuring Engine (PSE) Integration

PSE is the foundational technology that makes PBA's reliability possible.

*   **Runtime Enforcement:** PSE integrates into the LLM's generation loop, using the defined HSMs (both the main agent HSM and the nested state HSMs) to constrain the LLM's output at runtime.
*   **Guarantees:** This ensures structurally valid outputs for states and tool calls, and enforces valid transitions between agent states.

Understanding PSE concepts enhances your ability to customize PBA. [See PSE Documentation](https://docs.theproxycompany.com/pse/)

## 5. Model Context Protocol (MCP)

PBA supports dynamic tool integration at runtime using the Model Context Protocol (MCP).

*   **Adaptability:** Agents can connect to MCP servers to gain access to new tools without restarting or retraining.
*   **Reliability:** Tools loaded via MCP are integrated into the `ToolCallState` and benefit from the same PSE-guaranteed schema enforcement.

[Learn more about MCP Integration](../extending/model-context-protocol.md)

These core concepts work together to create an agent framework focused on engineered reliability, control, and adaptability.


---
pba-docs/docs/concepts/pse-integration.md
---
# PSE Integration: The Engine Behind Reliability

The Proxy Base Agent (PBA) achieves its reliability and stateful execution guarantees through its deep integration with the **Proxy Structuring Engine (PSE)**. While PBA defines the high-level agent workflow and states, PSE provides the low-level runtime enforcement mechanism.

## How PBA Uses PSE

PSE operates at two critical levels within PBA:

1.  **Enforcing Agent State Transitions:** The main `AgentStateMachine` (defining the Plan -> Act loop) is configured within PBA's `StructuringEngine` (provided by PSE). During generation, PSE's `process_logits` hook ensures that the LLM can only generate tokens that lead to valid state transitions according to the `AgentStateMachine`'s graph. This prevents the agent from getting stuck or taking unexpected paths.

2.  **Enforcing Structure within States:** Each individual `AgentState` (like `Thinking`, `ToolCallState`, `Python`) defines its *own* nested PSE `StateMachine`. This nested machine dictates the required structure for the content generated *while the agent is in that state*.
    *   For `ToolCallState`, this nested machine enforces that the output must be valid JSON matching the schema of one of the available tools.
    *   For `Python`, it ensures syntactically valid Python code.
    *   For planning states (`Thinking`, etc.), it typically enforces that the content is wrapped in the correct delimiters (e.g., ```thinking ... ```).

## The Guarantee

This dual enforcement means:

*   The agent **must** follow the defined high-level workflow (e.g., Plan before Acting).
*   The output generated *within* each state **must** conform to that state's structural requirements (e.g., valid tool call JSON).

PSE's runtime logit masking and state tracking provide the technical foundation for PBA's guarantees, transforming the LLM from a probabilistic text generator into a more reliable, controllable execution engine within the defined state machine boundaries.

## Further Reading

For a deeper understanding of the underlying mechanisms:

*   Refer to the [Proxy Structuring Engine Documentation](https://docs.theproxycompany.com/pse/) for details on PSE's core concepts (HSM, Steppers, Engine, Token Healing).

This tight integration ensures not only that the agent follows the correct high-level steps but also that each individual step (like a tool call or code generation) is structurally sound, drastically reducing runtime errors and increasing overall system reliability.

---
pba-docs/docs/concepts/state-machine.md
---
# State Machine

The Proxy Base Agent's (PBA) behavior is governed by an explicit **Hierarchical State Machine (HSM)**. Unlike traditional agents that rely heavily on prompt chaining, PBA uses this state machine, enforced by the underlying Proxy Structuring Engine (PSE), to ensure reliable and predictable execution flow.

## The Core Loop: Plan -> Act

The default PBA state machine defines a fundamental operational cycle:

1.  **Planning Phase:** The agent first evaluates the task. If planning is needed (or forced), it enters a planning loop. Within each loop iteration, it chooses one of the available planning states (`Thinking`, `Scratchpad`, `InnerMonologue`) to analyze the task, break down problems, or formulate strategy. The agent can cycle through this planning loop multiple times (configurable) to refine its approach. PSE ensures the content generated within these states adheres to their defined delimiters (e.g., ```thinking ... ```).

2.  **Action Phase:** Once planning is sufficient, the agent transitions to the action phase. It selects *one* available action state, such as `ToolCallState` (to use an external tool) or `Python` (to execute code). PSE guarantees that the output for the chosen action state conforms to the required structure (e.g., a valid JSON schema for the selected tool call).

3.  **Completion:** After executing the action, the agent typically transitions to a final state, often awaiting further user input or concluding the task.

```mermaid
flowchart TD
    Start([Start]) --> PlanLoopCheck{Evaluate Task}
    PlanLoopCheck -- Requires Planning --> Plan
    PlanLoopCheck -- Direct Action OK --> Action

    subgraph Plan["Planning Phase (Loop â‰¤ N times)"]
        direction LR
        PlanningChoice{"Choose planning type"} --> Thinking["Thinking"]
        PlanningChoice --> Scratchpad["Scratchpad"]
        PlanningChoice --> InnerMonologue["Inner Monologue"]
    end

    Plan --> PlanLoopDecision{"More planning needed?"}
    PlanLoopDecision -- "Yes" --> Plan
    PlanLoopDecision -- "No" --> Action

    subgraph Action["Action Phase"]
        direction LR
        ActionChoice{"Choose action type"} --> ToolAction["Tool Call"]
        ActionChoice --> CodeAction["Python Code"]
    end

    Action --> Finish([Finish/Await User])

    classDef phase fill:#DAD0AF,stroke:#0c5460,color:#024645
    classDef decision fill:#024645,stroke:#DAD0AF,color:#DAD0AF,shape:diamond
    classDef state fill:#024645,stroke:#DAD0AF,color:#DAD0AF
    classDef terminal fill:#024645,stroke:#DAD0AF,color:#DAD0AF,shape:stadium

    class Plan,Action phase
    class PlanLoopCheck,PlanLoopDecision,ActionChoice decision
    class PlanningChoice,Thinking,Scratchpad,InnerMonologue,ToolAction,CodeAction state
    class Start,Finish terminal
```

## Why an HSM?

Using an explicit HSM enforced by PSE provides key advantages:

*   **Reliability:** Prevents the agent from getting stuck, hallucinating invalid actions, or deviating from the intended workflow.
*   **Control:** Allows developers to precisely define and constrain the agent's behavior.
*   **Observability:** Makes the agent's internal state and decision-making process transparent.
*   **Extensibility:** Provides a clear structure for adding new states and capabilities.

This state machine architecture is fundamental to PBA's ability to perform complex tasks reliably. You can learn more about extending this base structure in the [Extending the Agent](../extending/index.md) section.

---
pba-docs/docs/concepts/states.md
---
# Agent States

Agent States are the building blocks of the Proxy Base Agent's (PBA) behavior, defined within its core [State Machine](./state-machine.md). Each state represents a distinct phase or capability within the agent's operational cycle.

## State Structure

Every `AgentState` in PBA typically includes:

*   **Identifier:** A unique machine-readable name (e.g., `thinking`, `tool_call`).
*   **Readable Name:** A human-friendly name for display (e.g., "Thinking", "External Tool Use").
*   **Delimiters:** A pair of strings (e.g., `("```thinking\n", "\n```")`) used by the agent to signal the start and end of content generated within that state.
*   **State Machine (via PSE):** A nested Proxy Structuring Engine (PSE) `StateMachine` that defines and enforces the *structure* of the content allowed *within* that state.
*   **State Prompt:** Instructions provided to the LLM explaining the purpose of the state and how to use its delimiters and structure.
*   **UI Properties:** Color and emoji for visual representation in interfaces.

## Default States in PBA

The standard PBA includes several pre-defined states grouped into Planning and Action phases:

### Planning States

These states are used within the agent's planning loop for internal reasoning and strategy formulation. The content generated here is typically *not* shown directly to the user.

*   **`Thinking`:** For deliberate, analytical thought processes (System 2 thinking). Simulates conscious reflection, reasoning about the task, and planning the next steps. Uses a `FencedFreeformStateMachine` to allow relatively unstructured text within its delimiters.
*   **`Scratchpad`:** For quick notes, temporary calculations, or outlining steps. Mimics jotting down ideas. Also uses a `FencedFreeformStateMachine`.
*   **`InnerMonologue`:** For more detailed, narrative-style internal dialogue. Allows the agent to explore nuances and build a coherent mental model. Uses a `FencedFreeformStateMachine`.

### Action States

These states are used when the agent needs to interact with the external environment or execute specific tasks after planning.

*   **`ToolCallState`:** The state for invoking external tools. Its internal PSE `StateMachine` is dynamically built based on the schemas of all currently available tools, ensuring the LLM generates a valid call structure (tool name + arguments matching one tool's schema).
*   **`Python`:** (Optional) Allows the agent to generate and request the execution of Python code snippets within a sandboxed environment. Uses a `PythonStateMachine` (via PSE's grammar types) wrapped in delimiters to ensure syntactically valid Python code.

## Role of PSE within States

Crucially, the nested PSE `StateMachine` associated with each `AgentState` enforces the structure *within* that state's output. For example:

*   The `ToolCallState`'s machine guarantees the output is valid JSON matching a known tool schema.
*   The `Python` state's machine guarantees the output is syntactically valid Python code.
*   The planning states' machines guarantee the output is correctly enclosed within the specified delimiters.

This ensures that even the agent's internal steps and action requests are structurally sound and reliable.

## Extending with Custom States

You can define and integrate your own custom `AgentState` classes to add unique capabilities or modify the agent's workflow. See [Custom States](../extending/custom-states.md).

---
pba-docs/docs/concepts/tools.md
---
# Tools

Tools are the primary mechanism through which the Proxy Base Agent (PBA) interacts with the external world, performs actions, and accesses capabilities beyond the core language model.

## What are Tools?

In PBA, a Tool represents a specific, callable function or capability. Examples include:

*   **API Wrappers:** Interacting with web services (e.g., weather, search, databases).
*   **Custom Functions:** Performing specific calculations, data processing, or logic defined by the developer.
*   **Code Execution:** Running code snippets (like the built-in `Python` state).
*   **User Interaction:** Sending messages back to the user (like the built-in `send_message` tool).

Each tool is defined with:

1.  **Name:** A unique identifier (e.g., `web_search`, `send_message`).
2.  **Description:** A natural language explanation of what the tool does and when to use it (used in the agent's prompt).
3.  **Schema:** A formal definition (typically JSON Schema derived from Python type hints or Pydantic models) specifying the input arguments the tool requires.
4.  **Implementation:** The actual Python code (callable) that executes the tool's logic.

## Reliable Tool Interaction via PSE

A key differentiator for PBA is how it handles tool interactions using the Proxy Structuring Engine (PSE):

1.  **Tool Selection:** During the Planning Phase, the agent reasons about which tool (if any) is needed to accomplish the current task.
2.  **Structured Invocation:** When the agent transitions to the `ToolCallState`, it must generate output conforming to the JSON schema of *one* of the available tools (including specifying its `intention` and the tool `name` and `arguments`). PSE *enforces* this structure at runtime.
3.  **Guaranteed Schema:** This means the agent *cannot* hallucinate non-existent tools or provide arguments that don't match the required types or format for the selected tool. Malformed tool calls are prevented *before* they happen.
4.  **Execution:** PBA receives the guaranteed-valid tool call structure (name and arguments) and executes the corresponding tool implementation.
5.  **Result Processing:** The tool's output (often formatted as an `Interaction` object) is returned to the agent's memory and informs the next step in its process.

This PSE-driven approach eliminates a major source of unreliability found in traditional agents, ensuring that tool calls are always structurally correct and executable.

## Adding Custom Tools

You can easily extend PBA's capabilities by adding your own custom tools. See the [Custom Tools](../extending/custom-tools.md) guide for details.

## Model Context Protocol (MCP)

PBA also supports dynamically loading tools at runtime from external servers using the Model Context Protocol (MCP). These tools are integrated seamlessly and benefit from the same PSE schema guarantees. See [MCP Integration](../extending/model-context-protocol.md).


---
pba-docs/docs/extending/custom-state-graphs.md
---
# Custom State Graphs

Modifying the core **State Graph** of the Proxy Base Agent (PBA) allows for fundamental changes to its operational workflow beyond just adding tools or individual states. This provides advanced control over the agent's execution loop, enabling complex custom behaviors.

## Understanding the State Graph

The main state graph is defined within the `__init__` method of the `agent.state_machine.AgentStateMachine` class. It's a Python dictionary where:

*   **Keys:** Represent the *origin* state's identifier (e.g., `"plan"`, `"take_action"`).
*   **Values:** Are lists of transitions originating from that state. Each transition is a tuple: `(StateMachine, TargetStateId)`.
    *   `StateMachine`: An instance of a PSE `StateMachine` (often an `AgentState`'s `.state_machine` property, or a composer like `LoopStateMachine` or `AnyStateMachine`) that governs the transition *and* the structure of the output generated during that transition.
    *   `TargetStateId`: The identifier of the state the agent will move to *after* successfully completing the transition governed by the `StateMachine`.

**Default Graph Structure (Simplified):**

```python
# Inside AgentStateMachine.__init__

planning_states = [...] # List of StateMachine instances for Thinking, etc.
action_states = [...]   # List of StateMachine instances for ToolCall, Python

state_graph = {
    "plan": [
        (
            LoopStateMachine( # Governs the planning loop
                AnyStateMachine(planning_states), # Allows any planning state
                min_loop_count=int(force_planning),
                max_loop_count=max_planning_loops,
                # ...
            ),
            "take_action", # Target state after planning loop completes
        )
    ],
    "take_action": [
        # Each tuple represents a possible action transition
        (action_state.state_machine, "done") for action_state in action_states
    ],
    # "done" is a terminal state (defined in end_states)
}

super().__init__(
    state_graph=state_graph,
    start_state="plan",
    end_states=["done"],
)
```

## Modifying the Graph

You can customize the agent's flow by directly editing this `state_graph` dictionary within `agent/state_machine.py`.

**Common Modifications:**

1.  **Changing the Planning Loop:**
    *   Adjust `min_loop_count` / `max_loop_count` in the `LoopStateMachine` within the `"plan"` state's transitions.
    *   Change the `AnyStateMachine(planning_states)` to a `ChainStateMachine` to enforce a specific *order* of planning states.
    *   Replace the planning loop entirely with a direct transition or a different structure.

2.  **Adding a New Top-Level State:**
    *   Define your new `CustomState` (see [Custom States](./custom-states.md)).
    *   Add a new key to the `state_graph` for your state's identifier (e.g., `"summarize"`).
    *   Define transitions *from* your new state (e.g., `("summarize", [(SummarizationState().state_machine, "done")])`).
    *   Modify existing transitions to point *to* your new state (e.g., change the target of the `"plan"` state's transition from `"take_action"` to `"summarize"`).
    *   Remember to add your `CustomState` instance to the `self.states` dictionary so the agent recognizes it.

3.  **Creating Conditional Transitions:**
    *   This is more advanced and typically involves creating a custom `StateMachine` subclass **in Python** (by inheriting from `pse_core.StateMachine` or composing base Python types like `ChainStateMachine`, `AnyStateMachine`, etc.) that implements logic to choose the `TargetStateId` based on the content generated or the agent's internal memory/context. The default PBA structure relies on the LLM choosing between parallel paths (like different tools in `take_action`).

4.  **Adding Parallel Action Paths:**
    *   Instead of `AnyStateMachine` for actions (implicitly handled by listing multiple transitions from `take_action`), you could define parallel structures if needed, though the default usually suffices as the LLM selects only one action path.

## Important Considerations

*   **PSE Knowledge:** Modifying the state graph effectively requires understanding how PSE `StateMachine` types (`Chain`, `Loop`, `Any`, etc.) compose and how transitions work. Refer to the [PSE Documentation](https://docs.theproxycompany.com/pse/).
*   **State Recognition:** Ensure any new state identifiers added to the graph keys or as `TargetStateId` values correspond to `AgentState` instances added to the `self.states` dictionary in `AgentStateMachine.__init__`.
*   **Prompting:** Update the system prompt (`agent/llm/prompts/base.txt` or your custom prompt) to accurately reflect the new workflow and instruct the LLM on how to navigate the modified state graph and use any new states.
*   **Complexity:** While powerful, overly complex state graphs can become difficult for the LLM to follow reliably, even with PSE's enforcement. Aim for clarity and logical flow.

Modifying the state graph offers deep control but should be done carefully, considering the impact on the agent's overall behavior and the LLM's ability to navigate the new structure.

---
pba-docs/docs/extending/custom-states.md
---
# Custom States

While PBA provides default states for Planning (`Thinking`, `Scratchpad`, `InnerMonologue`) and Action (`ToolCallState`, `Python`), you can create entirely new states to model unique phases or capabilities in your agent's workflow.

## Defining a Custom State

Creating a custom state involves subclassing the `agent.state.AgentState` abstract base class.

**Steps:**

1.  **Create a Python File:** Typically within the `agent/state/` directory or a custom subdirectory (e.g., `agent/state/custom/my_state.py`).
2.  **Subclass `AgentState`:** Define a new class inheriting from `AgentState`.
3.  **Implement `__init__`:**
    *   Call `super().__init__(...)` providing:
        *   `identifier`: A unique, lowercase string for the state (e.g., `"summarize"`).
        *   `readable_name`: A human-friendly name (e.g., `"Summarization"`).
        *   `delimiters`: A tuple of start and end strings (e.g., `("```summary\n", "\n```")`).
        *   `color`: A Rich color name for UI styling (e.g., `"blue"`).
        *   `emoji`: An emoji character for UI styling (e.g., `"scroll"`).
    *   Store any state-specific configuration.
4.  **Implement `state_machine` Property:**
    *   This property must return a configured PSE `StateMachine` instance. This machine defines and enforces the structure of the content generated *within* this custom state. You can use any PSE `StateMachine` type (e.g., `FencedFreeformStateMachine`, `JsonStateMachine`, `PythonStateMachine`, or a custom composition).
5.  **Implement `state_prompt` Property:**
    *   This property must return a string containing instructions for the LLM on *how* and *when* to use this state, including how to use the delimiters and expected content structure.

## Example: A Simple "Summarization" State

Let's create a state where the agent generates a concise summary.

```python
# agent/state/custom/summarization.py
from pse.types.misc.fenced_freeform import FencedFreeformStateMachine
from pse_core.state_machine import StateMachine
from agent.state import AgentState

class SummarizationState(AgentState):
    def __init__(self, delimiters: tuple[str, str] | None = None, character_max: int = 500):
        super().__init__(
            identifier="summarize",
            readable_name="Summarization",
            delimiters=delimiters or ("```summary\n", "\n```"),
            color="green",
            emoji="scroll",
        )
        self.character_max = character_max

    @property
    def state_machine(self) -> StateMachine:
        # Use FencedFreeform to allow text within delimiters, enforcing length
        sm = FencedFreeformStateMachine(
            identifier=self.identifier,
            delimiters=self.delimiters,
            char_min=10, # Require at least a short summary
            char_max=self.character_max,
        )
        # Important: Assign the identifier for PBA to recognize it
        sm.identifier = self.identifier
        return sm

    @property
    def state_prompt(self) -> str:
        return f"""
    The Summarization state is used to generate a concise summary of previous interactions or information.
    Keep the summary brief and to the point, adhering to the character limit ({self.character_max}).

    Always encapsulate the summary within {self.delimiters[0]!r} and {self.delimiters[1]!r} tags.
    This state's output might be shown to the user.
        """
```

## Integrating Custom States

To make the agent use your custom state, you need to modify the main `AgentStateMachine` definition in `agent/state_machine.py`:

1.  **Import:** Import your custom state class.
2.  **Instantiate:** Create an instance of your custom state.
3.  **Add to `self.states`:** Add the instance to the `self.states` dictionary in `AgentStateMachine.__init__` so the agent recognizes its identifier.
4.  **Modify State Graph:** Update the `state_graph` dictionary to include transitions *to* and *from* your new state. For example, you might add it as an option in the Planning phase or as a distinct step after the Action phase.

See [Custom State Graphs](./custom-state-graphs.md) for more details on modifying the agent's core workflow.


---
pba-docs/docs/extending/custom-tools.md
---
# Custom Tools

Adding custom tools is one of the primary ways to extend the capabilities of the Proxy Base Agent (PBA), allowing it to interact with specific APIs, databases, or perform specialized logic relevant to your application.

## Defining a Tool

A tool in PBA is essentially a Python function with type hints and a docstring, which PBA uses to automatically generate the necessary schema for the Proxy Structuring Engine (PSE).

**Steps to Create a Custom Tool:**

1.  **Create a Python File:** Create a new `.py` file in the `agent/tools/` directory (or a subdirectory). The filename (excluding `.py`) will typically be used as the tool's name.
2.  **Define the Function:** Write a standard Python function (it can be `async` or synchronous).
    *   **First Argument:** The function *must* accept `self: Agent` as its first argument. This provides access to the agent's state and methods if needed.
    *   **Type Hinting:** Use standard Python type hints for all other arguments and the return type. PBA uses these hints (along with Pydantic if models are used) to generate the JSON Schema for PSE.
    *   **Docstring:** Write a clear docstring explaining what the tool does, its arguments, and what it returns. The first line is often used as a short description, and the rest provides details.
3.  **Return Value:** The function should ideally return an `agent.system.interaction.Interaction` object, typically with `role=Interaction.Role.TOOL`. This allows you to structure the tool's output clearly, including content, titles, or even images. Returning a simple string is also possible, which will be wrapped in a basic `Interaction` object.

## Example: Simple Calculator Tool

Let's create a tool named `calculator` that adds two numbers.

1.  **Create File:** `agent/tools/calculator.py`

2.  **Define Function:**

    ```python
    # agent/tools/calculator.py
    from agent.agent import Agent
    from agent.system.interaction import Interaction

    async def calculator(
        self: Agent,
        num1: float,
        num2: float,
        operation: str = "add"
    ) -> Interaction:
        """
        Performs basic arithmetic operations (add, subtract, multiply, divide).

        Args:
            num1: The first number.
            num2: The second number.
            operation: The operation to perform ('add', 'subtract', 'multiply', 'divide'). Defaults to 'add'.

        Returns:
            An Interaction object containing the calculation result or an error message.
        """
        result: float | str
        try:
            if operation == "add":
                result = num1 + num2
            elif operation == "subtract":
                result = num1 - num2
            elif operation == "multiply":
                result = num1 * num2
            elif operation == "divide":
                if num2 == 0:
                    raise ValueError("Cannot divide by zero.")
                result = num1 / num2
            else:
                raise ValueError(f"Unsupported operation: {operation}")

            content = f"Calculation result: {num1} {operation} {num2} = {result}"
            color = "green"
            emoji = "abacus"

        except ValueError as e:
            content = f"Calculation error: {e}"
            color = "red"
            emoji = "warning"

        return Interaction(
            role=Interaction.Role.TOOL,
            content=content,
            title="Calculator Result",
            color=color,
            emoji=emoji,
        )

    ```

## Integration with PBA

PBA automatically discovers tools placed in the `agent/tools/` directory when it starts (specifically, via `Tool.load()` called within `Agent.__init__`).

1.  **Automatic Loading:** When the `Agent` is initialized, it scans the tools directory.
2.  **Schema Generation:** For each discovered function (like `calculator`), PBA uses PSE's capabilities (`callable_to_schema`) to generate a JSON Schema based on the function's signature (arguments, type hints) and docstring.
3.  **State Machine Update:** The generated schema is added to the list of available tools within the `ToolCallState`. The `AgentStateMachine` (and thus the underlying PSE engine) is configured with this updated list.
4.  **Prompt Update:** The tool's name, description (from the docstring), and schema are automatically included in the system prompt section describing available tools.

Now, when the agent decides it needs to perform a calculation during its Planning Phase, it can generate a `ToolCall` targeting the `calculator` tool within the `ToolCallState`. PSE will ensure the generated call includes `num1`, `num2`, and optionally `operation` with the correct types, guaranteeing a valid call to your Python function.

## Next Steps

*   Explore the existing tools in `agent/tools/` for more examples.
*   Consider using Pydantic models for complex tool arguments; PBA integrates seamlessly with them for schema generation.
*   Learn about adding tools dynamically via [MCP](../extending/model-context-protocol.md).


---
pba-docs/docs/extending/index.md
---
# Extending the Proxy Base Agent

The Proxy Base Agent (PBA) is designed as a foundation. While it provides a robust core loop for planning and action, its true power lies in its extensibility. You can tailor PBA to specific tasks and domains by adding custom capabilities and modifying its core behavior.

## Key Extension Points

There are several ways to extend and customize PBA:

1.  **[Custom Tools](./custom-tools.md):**
    *   **What:** Define new Python functions that the agent can call to interact with external APIs, databases, or perform specialized computations.
    *   **Why:** This is the most common way to add new capabilities and ground the agent in specific data or services relevant to your application.
    *   **How:** Create a Python file in `agent/tools/`, define your function with type hints and a docstring. PBA automatically discovers it, generates a schema, and makes it available to the `ToolCallState`.

2.  **[Custom States](./custom-states.md):**
    *   **What:** Define entirely new `AgentState` classes with their own unique logic, prompts, delimiters, and internal PSE `StateMachine` for structure enforcement.
    *   **Why:** Allows you to add distinct phases or modes of operation to the agent beyond the default Planning/Action states (e.g., a "Summarization" state, a "User Feedback" state).
    *   **How:** Subclass `agent.state.AgentState`, implement the required properties (`state_machine`, `state_prompt`), and integrate it into the main `AgentStateMachine`.

3.  **[Custom State Graphs](./custom-state-graphs.md):**
    *   **What:** Modify the main `AgentStateMachine` definition in `agent/state_machine.py`.
    *   **Why:** Change the agent's core workflow. You could alter the planning loop, add parallel action paths, introduce conditional transitions, or create entirely different high-level agent architectures.
    *   **How:** Directly edit the `state_graph` dictionary within the `AgentStateMachine` class, defining new states and transitions using existing or custom `AgentState` instances. Requires understanding PSE `StateMachine` composition.

4.  **[Model Context Protocol (MCP)](./model-context-protocol.md):**
    *   **What:** Connect the agent to external MCP servers at runtime.
    *   **Why:** Dynamically load tools and capabilities from other services without modifying the agent's core code. Enables building distributed, adaptive agent systems.
    *   **How:** Use the built-in `list_mcp_servers` and `add_mcp_server` tools. PBA handles the connection and dynamic reconfiguration of the `ToolCallState`.

## Choosing the Right Extension Method

*   For adding specific actions or API interactions: Start with **Custom Tools**.
*   For adding new distinct phases or modes to the agent's workflow: Use **Custom States** and modify the **State Graph**.
*   For fundamentally changing the agent's core execution loop: Modify the **State Graph**.
*   For integrating external, dynamically available capabilities: Use **MCP**.

By leveraging these extension points, you can transform the Proxy Base Agent from a general foundation into a specialized agent tailored precisely to your needs.

---
pba-docs/docs/extending/model-context-protocol.md
---
# Model Context Protocol (MCP) Integration

The Proxy Base Agent (PBA) supports dynamic extension of its capabilities at runtime through the **Model Context Protocol (MCP)**. MCP allows PBA to connect to external servers that offer specialized tools and functionalities.

## What is MCP?

MCP is a standardized protocol designed for language models and agents to interact with external services and tools securely and efficiently. An MCP server exposes a set of tools, each with a defined schema, that connected agents can invoke.

Think of MCP servers as plug-and-play capability providers for your agent.

## How PBA Uses MCP

PBA integrates with MCP through a built-in workflow facilitated by specific tools and internal mechanisms:

1.  **Discovery (`list_mcp_servers` Tool):** The agent can use the built-in `list_mcp_servers` tool to discover available MCP servers defined in its configuration (typically `agent/mcp/servers/servers_list.json`). This tool returns information about each server, including its name, description, and identifier.
2.  **Connection (`add_mcp_server` Tool):** Based on its planning and the information from `list_mcp_servers`, the agent can decide to connect to a specific server using the built-in `add_mcp_server` tool, providing the server's unique identifier.
3.  **Runtime Integration:** When `add_mcp_server` is called:
    *   PBA's `MCPHost` establishes a connection to the server using an `MCPClient`.
    *   The agent retrieves the list of tools offered by that server.
    *   These external tools are converted into standard PBA `Tool` objects.
    *   The agent calls its internal `add_tools()` method.
    *   Crucially, `add_tools()` triggers `agent.configure()`, which **rebuilds the `AgentStateMachine` and reconfigures the underlying PSE `StructuringEngine`** with the updated list of tools (including the newly added ones from the MCP server).
4.  **Reliable Usage:** Once connected and configured, the tools from the MCP server are seamlessly available within the agent's `ToolCallState`. The agent can generate calls to these tools, and PSE provides the **same guarantee of structural validity** for these dynamically added tools as it does for locally defined tools.

## Benefits of MCP Integration

*   **Dynamic Adaptability:** Add or remove agent capabilities without restarting or modifying the agent's core code.
*   **Extensibility:** Easily integrate specialized third-party services or internal microservices.
*   **Modularity:** Keep specialized logic separate from the core agent framework.
*   **Reliability:** Dynamically added tools still benefit from PSE's schema enforcement guarantees.

## Managing Servers

*   Available servers are defined in `agent/mcp/servers/servers_list.json`. You can add definitions for custom or private MCP servers here.
*   MCP servers may require specific environment variables for authentication or configuration. PBA attempts to pass these from the agent's environment to the server process during connection (see `agent/mcp/client.py`). Ensure necessary variables are set in the agent's environment.

MCP provides a powerful mechanism for creating adaptive and highly capable agents by decoupling core logic from specialized, dynamically loaded functionalities.

---
pba-docs/docs/api/agent.md
---
# Agent Class

The `agent.agent.Agent` class is the central component of the Proxy Base Agent framework. It orchestrates the interaction between the language model, the defined state machine, tools, memory, and the user interface.

```python
class Agent:
    def __init__(
        self,
        name: str,
        system_prompt_name: str,
        interface: Interface,
        inference: LocalInference,
        seed: int | None = None,
        tools: list[Tool] | list[str] | None = None,
        python_interpreter: bool = False,
        max_planning_loops: int = 3,
        force_planning: bool = True,
        character_max: int | None = None,
        include_pause_button: bool = True,
        **inference_kwargs,
    ):
        # ... implementation ...
```

## Initialization (`__init__`)

The constructor initializes the agent with its core configuration.

**Parameters:**

*   `name` (`str`): A human-readable name for this specific agent instance (e.g., "ResearchAssistant", "CodeHelper").
*   `system_prompt_name` (`str`): The filename (without extension) of the system prompt template located in `agent/llm/prompts/`. This prompt defines the agent's core instructions, personality, and includes placeholders for state machine details and tools.
*   `interface` (`Interface`): An instance of a class implementing the `agent.interface.Interface` abstract base class (e.g., `CLIInterface`). This handles all input/output with the user.
*   `inference` (`LocalInference`): An instance of `agent.llm.local.LocalInference`, which manages the connection to the local LLM backend (via a `Frontend`) and holds the `StructuringEngine`.
*   `seed` (`int | None`, optional): A seed for the random number generator used during LLM sampling, allowing for reproducible outputs. Defaults to a random integer if `None`.
*   `tools` (`list[Tool] | list[str] | None`, optional): Specifies the tools available to the agent. Can be a list of instantiated `Tool` objects, a list of tool filenames (strings) to load from `agent/tools/`, or `None` to automatically load all tools found in the default `agent/tools/` directory. Defaults to `None` (load all).
*   `python_interpreter` (`bool`, optional): If `True`, enables the `Python` action state, allowing the agent to generate and execute Python code snippets. Defaults to `False`.
*   `max_planning_loops` (`int`, optional): The maximum number of times the agent can cycle through its planning states (`Thinking`, `Scratchpad`, `InnerMonologue`) before being forced to transition to the action phase. Defaults to `3`.
*   `force_planning` (`bool`, optional): If `True`, the agent *must* complete at least one planning loop before taking action. If `False`, the agent can potentially skip planning and go directly to an action if the LLM deems it appropriate. Defaults to `True`.
*   `character_max` (`int | None`, optional): An approximate maximum character limit enforced within certain states (like planning states) via the underlying PSE `StateMachine`. Defaults to `None` (often handled by state-specific defaults).
*   `include_pause_button` (`bool`, optional): If `True`, sets up a keyboard listener (using `pynput`) to allow pausing/resuming agent generation by pressing the spacebar. Defaults to `True`.
*   `**inference_kwargs`: Additional keyword arguments passed directly to the `LocalInference` instance and subsequently to the LLM backend during generation (e.g., `temp`, `max_tokens`, `cache_system_prompt`).

## Key Methods

### `async loop()`

Starts the main interactive loop of the agent.

1.  Prompts the user for input via the configured `interface`.
2.  Adds the user's message to the agent's `memory`.
3.  Enters a processing cycle (`while self.can_act:`):
    *   Calls `generate_action()` to get the next structured output from the LLM (guided by PSE and the `AgentStateMachine`).
    *   Calls `take_action()` to interpret the structured output, execute the corresponding logic (log planning state, call tool, run Python), and update memory.
    *   Increments the internal step counter.
4.  Repeats the processing cycle until `self.can_act` becomes `False` (e.g., max steps reached, or an action state signals completion like `send_message` with `wait_for_response=True`).
5.  Recursively calls `loop()` to wait for the next user input.

### `configure(set_system_prompt: bool = False)`

(Re)configures the agent's state machine and PSE engine. This is called initially during `__init__` and also whenever tools are added/removed (e.g., via MCP).

1.  Creates a new `AgentStateMachine` instance based on the current set of `tools`, `python_interpreter` setting, and planning parameters (`max_planning_loops`, `force_planning`).
2.  Configures the underlying `StructuringEngine` (`self.inference.engine`) with this new `AgentStateMachine`.
3.  Optionally updates the system prompt in the agent's `memory` if `set_system_prompt` is `True`.

### `add_tools(new_tools: list[Tool], reset_system_prompt: bool = False)`

Adds new tools to the agent's available toolset.

1.  Updates the internal `self.tools` dictionary.
2.  Calls `configure(reset_system_prompt=reset_system_prompt)` to rebuild the `AgentStateMachine`, reconfigure the PSE engine with the updated tool schemas, and optionally refresh the system prompt in memory to include the new tools.

*(Other methods like `generate_action`, `take_action`, `use_tool` handle the internal processing steps within the loop.)*

---
pba-docs/docs/api/index.md
---
# API Reference

This section provides detailed reference documentation for the core classes and components of the Proxy Base Agent (PBA).

Understanding these APIs is essential for extending PBA, integrating it into larger systems, or customizing its behavior at a code level.

## Key Components

*   **[Agent](./agent.md):** The main orchestrator class managing the agent's lifecycle, state, memory, tools, and interaction with the LLM backend.
*   **[Tool & ToolCall](./tool.md):** Classes defining external capabilities (`Tool`) and the structure for invoking them (`ToolCall`).
*   **[AgentState](./state.md):** The base class for defining individual states within the agent's state machine and the default states provided by PBA.

*(Note: This API reference is currently under development. More detailed documentation for each component will be added.)*

---
pba-docs/docs/api/state.md
---
# AgentState API Reference

The `agent.state.AgentState` class is the abstract base class for defining individual states within the Proxy Base Agent's (PBA) state machine.

## `AgentState` (Abstract Base Class)

All specific states (like `Thinking`, `ToolCallState`, etc.) inherit from this class.

```python
from abc import ABC, abstractmethod
from pse_core.state_machine import StateMachine

class AgentState(ABC):
    def __init__(
        self,
        identifier: str,
        readable_name: str,
        delimiters: tuple[str, str],
        color: str,
        emoji: str,
    ):
        self.identifier = identifier
        self.readable_name = readable_name
        self.delimiters = delimiters
        self.color = color
        self.emoji = emoji

    @property
    @abstractmethod
    def state_machine(self) -> StateMachine:
        # Must return a configured PSE StateMachine
        pass

    @property
    @abstractmethod
    def state_prompt(self) -> str:
        # Must return instructions for the LLM
        pass

    def format(self, string: str) -> str:
        # Helper to wrap content in delimiters
        return f"{self.delimiters[0]}{string}{self.delimiters[1]}"

    def readable_format(self, string: str) -> str:
        # Helper for UI formatting (default: markdown code block)
        return f"```markdown\n{string}\n```"

    def __str__(self) -> str:
        # Used for generating the system prompt section
        return f"{self.readable_name.title()}: {self.state_prompt}"

```

**Key Attributes:**

*   `identifier` (`str`): Unique machine-readable name (e.g., `"thinking"`). Used as keys in the main `AgentStateMachine` graph and for identifying output segments.
*   `readable_name` (`str`): Human-friendly name for UI display (e.g., `"Thinking"`).
*   `delimiters` (`tuple[str, str]`): Start and end strings the LLM uses to enclose content generated for this state. PSE uses these within the nested `state_machine`.
*   `color` (`str`): [Rich](https://rich.readthedocs.io/en/latest/style.html#color) color name for styling in the CLI.
*   `emoji` (`str`): Emoji character for styling in the CLI.

**Abstract Properties (Must be Implemented by Subclasses):**

*   `state_machine` (`property -> StateMachine`): Must return a configured instance of a `pse_core.StateMachine` (or a subclass like `FencedFreeformStateMachine`, `JsonStateMachine`, etc.). This machine defines and enforces the structure of the content *within* this state. It's crucial to set `sm.identifier = self.identifier` on the returned state machine instance so PBA can correctly associate generated output segments.
*   `state_prompt` (`property -> str`): Must return a string containing instructions for the LLM. This text is incorporated into the main system prompt and should explain the purpose of this state, how to use its delimiters, and the expected content/structure.

## Default PBA States

PBA provides the following built-in states inheriting from `AgentState`:

**Planning States:**

*   `agent.state.planning.thinking.Thinking`: For deliberate reasoning. Uses `FencedFreeformStateMachine`.
*   `agent.state.planning.scratchpad.Scratchpad`: For temporary notes. Uses `FencedFreeformStateMachine`.
*   `agent.state.planning.inner_monologue.InnerMonologue`: For detailed internal narrative. Uses `FencedFreeformStateMachine`.

**Action States:**

*   `agent.state.action.tool_call.ToolCallState`: For invoking tools. Dynamically builds a `JsonStateMachine` based on available tool schemas.
*   `agent.state.action.python.Python`: For executing Python code. Uses an `EncapsulatedStateMachine` wrapping a `PythonStateMachine` (from PSE grammar types).

Refer to the source code of these classes for examples of how to implement the `state_machine` and `state_prompt` properties.

---
pba-docs/docs/api/tool.md
---
# Tool & ToolCall API Reference

These classes define how external capabilities (Tools) are represented and invoked within the Proxy Base Agent (PBA).

## `agent.tools.Tool`

Represents a callable capability available to the agent.

```python
class Tool:
    def __init__(
        self,
        name: str,
        description: str | None = None,
        callable: Callable | None = None,
        schema: dict[str, Any] | None = None,
        mcp_server: str | None = None,
    ):
        # ... implementation ...

    async def call(self, caller: Any, **kwargs) -> Any:
        # ... implementation ...

    @staticmethod
    def from_file(filepath: str) -> Tool | None:
        # ... implementation ...

    @staticmethod
    def load(
        filepath: str | None = None,
        file_name: str | list[str] | None = None,
    ) -> list[Tool]:
        # ... implementation ...

    def to_dict(self) -> dict[str, Any]:
        # ... implementation ...

    @staticmethod
    def from_mcp_tool(mcp_tool: MCPTool, server_id: str) -> Tool:
        # ... implementation ...
```

**Initialization (`__init__`)**

*   `name` (`str`): The unique identifier for the tool (e.g., `calculator`, `send_message`). This name is used by the LLM in the `ToolCallState`.
*   `description` (`str | None`, optional): A natural language description of the tool's purpose and usage. Used in the system prompt. If derived from a callable, this is often taken from the docstring.
*   `callable` (`Callable | None`, optional): The actual Python function (sync or async) that implements the tool's logic. If provided, the `schema` is usually auto-generated.
*   `schema` (`dict[str, Any] | None`, optional): A JSON Schema dictionary describing the tool's input parameters (`arguments`). If `callable` is provided, this is auto-generated using `callable_to_schema`. If `callable` is *not* provided (e.g., for an MCP tool where the implementation is remote), this schema *must* be provided.
*   `mcp_server` (`str | None`, optional): If this tool is provided by an external MCP server, this attribute stores the server's identifier.

**Key Methods**

*   `async call(self, caller: Any, **kwargs) -> Any`: Executes the tool's underlying `callable`. It handles both synchronous and asynchronous functions. The `caller` argument (which is the `Agent` instance invoking the tool) is automatically passed as the *first* argument (conventionally named `self`) to the tool's `callable` function. `**kwargs` are the arguments extracted from the LLM's tool call, matching the tool's defined `schema`.
*   `to_dict() -> dict`: Generates a JSON Schema representation suitable for inclusion in the `ToolCallState`'s schema definition and the system prompt. It wraps the tool's argument `schema` within a structure that also requires an `intention` field from the LLM.
*   `load(...)` (staticmethod): Discovers and loads tools from Python files in a specified directory (defaults to `agent/tools/`).
*   `from_file(...)` (staticmethod): Loads a single tool from a specific Python file.
*   `from_mcp_tool(...)` (staticmethod): Converts a tool definition received via the MCP protocol into a PBA `Tool` instance.

## `agent.tools.ToolCall`

A Pydantic `BaseModel` representing a structured request from the LLM to invoke a tool. PSE guarantees that the output within the `ToolCallState` conforms to this structure (or specifically, the structure generated by `Tool.to_dict()`).

```python
class ToolCall(BaseModel):
    intention: str
    name: str
    arguments: dict[str, Any] | None = None```

**Fields:**

*   `intention` (`str`): A description generated by the LLM explaining *why* it is calling the tool and what it intends to achieve. This provides valuable context for understanding the agent's reasoning.
*   `name` (`str`): The exact name of the `Tool` to be invoked. Must match the `name` attribute of one of the loaded `Tool` instances.
*   `arguments` (`dict[str, Any] | None`, optional): A dictionary containing the arguments for the tool call. The structure and types within this dictionary must conform to the `schema` defined for the specified `Tool`. PSE enforces this conformance.

PBA uses the `ToolCall` model to parse the validated JSON output from the `ToolCallState` before passing the `name` and `arguments` to the `Agent.use_tool` method for execution.

---
pse-docs/docs/index.md
---
# Proxy Structuring Engine

The **Proxy Structuring Engine** (PSE) is a system for dynamically constrained natural language generation.

It compiles rules and schemas into efficient hierarchical state machines that dynamically filter token probabilities during generation, guaranteeing structurally valid outputs while preserving natural language fluency.

## Use Cases
- **Tool Calling** - Generate precise, validated parameters for function calls
- **API Integration** - Guarantee well-formed outputs for seamless system interoperability
- **Synthetic Data** - Create diverse, schema-conformant datasets for training
- **Structured Output** - Enforce type-safe results for reliable downstream processing
- **Agent Frameworks** - Constrain agent actions and reasoning (see [Proxy Base Agent](https://github.com/TheProxyCompany/proxy-base-agent))

## Key Points
- **Dynamic Validation** - A hierarchical state machine validates each token generated, ensuring correctness during generation
- **Token Healing** - Automatically recovers from tokenization mismatches, maintaining structural integrity
- **Parallel Generation** - Explores multiple potential output paths concurrently, maximizing quality within constraints
- **Minimal Overhead** - Adds less than 20ms per token, making it suitable for latency-sensitive applications
- **Schema Versatility** - Supports JSON Schema, Pydantic models, custom grammars, and custom state machines for maximum flexibility
- **Framework Agnostic** - Integrates with PyTorch, MLX, TensorFlow, JAX, and more

## Getting Started

```python
from pse import StructuringEngine
from pydantic import BaseModel
from transformers import AutoTokenizer

# 1. Define your desired output structure
class User(BaseModel):
    name: str
    age: int

# 2. Initialize the tokenizer and engine
tokenizer = AutoTokenizer.from_pretrained("your-model-name")
engine = StructuringEngine(tokenizer)

# 3. Configure the engine with your schema
engine.configure(User)

# 4. Integrate with your generation loop
# (Example using Hugging Face Transformers)
output = model.generate(
    ...,
    logits_processor=[engine.process_logits],
    sampler=engine.sample, # Use the engine's sampling method for multi-token generation
)

# 5. Extract the structured output
structured_user = engine.get_structured_output(User, raise_on_error=True)
print(structured_user)

```

## Open Source
PSE is released under the Apache 2.0 license.

We enthusiastically welcome community contributions and collaborations.

[Explore the Code](https://github.com/TheProxyCompany/proxy-structuring-engine){: .md-button .md-button--primary }


---
pse-docs/docs/getting-started/installation.md
---
# Installation

This guide will help you install the Proxy Structuring Engine (PSE) and set up your environment for development.

## Standard Installation

We recommend using uv to install PSE:
```bash
uv pip install pse
```

*(This installs the `pse` Python library and its required dependency `pse-core`, which contains the pre-compiled C++ engine.)*

You can also install PSE using pip:

```bash
pip install pse
```


## Development Installation

For development or to access the latest features, you can install from source:

```bash
# Clone the repository
git clone https://github.com/TheProxyCompany/proxy-structuring-engine.git
cd proxy-structuring-engine

# Install in development mode
pip install -e ".[dev]"
```

## Framework-Specific Installation

PSE works with multiple ML frameworks. Install the one(s) you plan to use:

### PyTorch

```bash
pip install pse[torch]

# Or for a specific version
pip install torch pse
```

### MLX (Apple Silicon)

```bash
pip install pse[mlx]

# Or for a specific version
pip install mlx pse
```

### TensorFlow

```bash
pip install pse[tensorflow]

# Or for a specific version
pip install tensorflow pse
```

### JAX

```bash
pip install pse[jax]

# Or for a specific version
pip install jax jaxlib pse
```

## System Requirements

- **Python**: 3.10 or higher
- **Operating Systems**:
  - Linux (Ubuntu 20.04+, Debian 11+, etc.)
  - macOS (11.0+)
  - Windows 10/11
- **Hardware**:
  - Any system capable of running your selected LLM (the PSE only works with local models)

## Getting Help

If you continue to experience issues:

- Check our [GitHub Issues](https://github.com/TheProxyCompany/proxy-structuring-engine/issues)

## Next Steps

Now that you've installed PSE, proceed to the [Quickstart](quickstart.md) guide to run your first example.


---
pse-docs/docs/getting-started/quickstart.md
---
# Quickstart: Pydantic to Guaranteed JSON

This guide demonstrates how to use the Proxy Structuring Engine (PSE) with a Hugging Face `transformers` model (using PyTorch) to generate JSON output guaranteed to match a Pydantic schema.

```python
import torch
from transformers import AutoTokenizer, LlamaForCausalLM
from pydantic import BaseModel

# Assuming PSE is installed:
from pse import StructuringEngine
from pse.util.torch_mixin import PSETorchMixin # Optional: Mixin for easy HF integration

# 1. Define your desired output structure using Pydantic
class UserProfile(BaseModel):
    user_id: int
    username: str
    is_active: bool
    roles: list[str]

# 2. (Optional) Apply the PSE mixin to your Hugging Face model class
#    This simplifies integration by adding the `engine` attribute and overriding `generate`.
class PSE_Llama(PSETorchMixin, LlamaForCausalLM):
    pass

# 3. Load your model and tokenizer
#    Replace with your desired model path.
model_path = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Use the mixed-in class or your base model class
model = PSE_Llama.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16, # Use appropriate dtype for your model/hardware
    device_map="auto"           # Load model efficiently across devices
)

# Ensure padding token is set for generation (important!)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
# Ensure the model config also reflects the pad token id
if model.config.pad_token_id is None:
     model.config.pad_token_id = tokenizer.pad_token_id

# 4. Create the StructuringEngine instance.
#    If using the mixin, it's automatically attached as `model.engine`.
#    If not using the mixin, instantiate it separately:
#    engine = StructuringEngine(tokenizer)
model.engine = StructuringEngine(tokenizer) # Assumes mixin usage for simplicity

# 5. Configure the engine with your Pydantic model.
#    PSE compiles this into an efficient HSM representation.
model.engine.configure(UserProfile)

# 6. Create your prompt, instructing the LLM to generate the desired structure.
prompt = f"Generate a user profile for user ID 999, username 'tester', active status true, roles ['qa', 'dev']. Output ONLY the JSON object."
messages = [{"role": "user", "content": prompt}]
input_ids = tokenizer.apply_chat_template(
    messages,
    return_tensors="pt",
    add_generation_prompt=True # Crucial for instruction-following models
).to(model.device)

# 7. Generate using the engine's processor and sampler.
#    If using the mixin, `model.generate` is already overridden.
#    If not using the mixin, pass the engine hooks manually:
#    output_ids = model.generate(
#        input_ids,
#        max_new_tokens=150,
#        do_sample=True, # Or False for greedy decoding
#        logits_processor=[engine.process_logits],
#        sampler=engine.sample
#    )
output_ids = model.generate(
    input_ids,
    max_new_tokens=150,
    do_sample=True # Example: using sampling
    # No need to pass hooks explicitly if using the mixin
)

# 8. Decode and parse the guaranteed structured output
#    Extract only the newly generated tokens, excluding the prompt.
output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)
print("Raw Output (Guided by PSE):\n", output_text)

# PSE guarantees this output can be parsed directly into your Pydantic model
# Use the engine instance (model.engine if using mixin, or your separate instance)
structured_output: UserProfile = model.engine.get_structured_output(UserProfile)

# Verify the output
if structured_output:
    print("\nParsed Pydantic Object:\n", structured_output)
    # Example Parsed Output:
    # UserProfile(user_id=999, username='tester', is_active=True, roles=['qa', 'dev'])
else:
    print("\nFailed to generate structured output.")

```

This example shows the basic workflow: Define -> Configure -> Generate -> Parse. PSE ensures the `structured_output` reliably conforms to the `UserProfile` Pydantic model.

---
pse-docs/docs/guides/base-state-machines.md
---
# Base State Machines

While PSE excels at automatically converting Pydantic models, JSON Schemas, and function signatures into the necessary `StateMachine` structures, it also provides a set of fundamental, composable `StateMachine` base types. These allow developers to define custom grammars or control flows directly, offering more granular control when needed.

These base types are located in `pse.types.base` and are the building blocks used internally by the schema conversion logic. Understanding them is key to advanced usage and creating bespoke structuring logic.

## Core Base Types

*   **`PhraseStateMachine`**:
    *   **Purpose:** Matches an exact sequence of characters (a specific string).
    *   **Use Case:** Defining keywords, delimiters, operators, or fixed string literals within a grammar.
    *   **Example:** `PhraseStateMachine("true")`, `PhraseStateMachine("```json\n")`

```python
# Example Usage: Match the keyword "class"
keyword_sm = PhraseStateMachine("class")

# Example Usage: Match a specific delimiter
delimiter_sm = PhraseStateMachine("---")
```

*   **`CharacterStateMachine`**:
    *   **Purpose:** Matches characters based on inclusion/exclusion rules (whitelist, blacklist, graylist) with optional length constraints (min/max characters).
    *   **Use Case:** Defining patterns like integers (`whitelist_charset="0123456789"`), whitespace (`whitelist_charset=" \t\n\r"`), or general string content excluding specific characters (`blacklist_charset='"'`).
    *   **Example:** `CharacterStateMachine(whitelist_charset="abc", char_limit=5)`

```python
# Example Usage: Match any digit
digit_sm = CharacterStateMachine(whitelist_charset="0123456789")

# Example Usage: Match whitespace (optional, up to 10 chars)
whitespace_sm = CharacterStateMachine(
    whitelist_charset=" \t\n\r",
    char_limit=10,
    is_optional=True
)

# Example Usage: Match any character except quotes
non_quote_sm = CharacterStateMachine(blacklist_charset='"')
```

*   **`ChainStateMachine`**:
    *   **Purpose:** Executes a sequence of other `StateMachine`s in a specific, fixed order. Each machine in the chain must complete successfully before the next one starts.
    *   **Use Case:** Defining structures where elements must appear sequentially, like a key-value pair (`StringStateMachine` -> `WhitespaceStateMachine` -> `PhraseStateMachine(":")` -> `WhitespaceStateMachine` -> `JsonStateMachine`).
    *   **Example:** `ChainStateMachine([PhraseStateMachine("A"), PhraseStateMachine("B")])` requires "AB".

```python
# Example Usage: Match "key: value" pattern
key_value_sm = ChainStateMachine([
    StringStateMachine(),     # Key
    PhraseStateMachine(":"),  # Separator
    WhitespaceStateMachine(), # Optional space
    JsonStateMachine()        # Value
])
```

*   **`AnyStateMachine`**:
    *   **Purpose:** Accepts input that matches *any one* of the provided `StateMachine`s. It essentially represents a logical OR condition.
    *   **Use Case:** Defining points in a grammar where multiple different structures are valid (e.g., a JSON value can be a string OR a number OR an object...). Used heavily in handling `anyOf` / `oneOf` in JSON Schema or `Union` types in Pydantic.
    *   **Example:** `AnyStateMachine([StringStateMachine(), NumberStateMachine()])` accepts either a string or a number.

```python
# Example Usage: Accept 'true', 'false', or 'null'
json_literal_sm = AnyStateMachine([
    PhraseStateMachine("true"),
    PhraseStateMachine("false"),
    PhraseStateMachine("null")
])
```

*   **`LoopStateMachine`**:
    *   **Purpose:** Repeats a single `StateMachine` a specified number of times (with optional min/max loop counts). Can include an optional separator `StateMachine` (like whitespace or a comma) between repetitions.
    *   **Use Case:** Defining arrays/lists where an element pattern repeats, potentially separated by commas or whitespace.
    *   **Example:** `LoopStateMachine(JsonStateMachine(), min_loop_count=1, whitespace_seperator=True)` matches one or more JSON values separated by optional whitespace.

```python
# Example Usage: Match one or more words separated by whitespace
# Assumes CharacterStateMachine is defined elsewhere
word_loop_sm = LoopStateMachine(
    state_machine=CharacterStateMachine(blacklist_charset=" \t\n\r"), # Matches non-whitespace chars
    min_loop_count=1,
    whitespace_seperator=True # Allows whitespace between words
)
# This would match "word1 word2 word3"
```

*   **`EncapsulatedStateMachine`**:
    *   **Purpose:** Matches content that is wrapped by specific start and end delimiter strings. It uses an inner `StateMachine` to validate the content *between* the delimiters.
    *   **Use Case:** Parsing content within specific tags or fences, like markdown code blocks (` ``` `) or custom XML-like tags.
    *   **Example:** `EncapsulatedStateMachine(JsonStateMachine(), delimiters=("<json>", "</json>"))` matches valid JSON enclosed in `<json>` tags.

```python
# Example Usage: Match JSON within markdown code fences
# Assumes JsonStateMachine is defined elsewhere
json_block_sm = EncapsulatedStateMachine(
    state_machine=JsonStateMachine(),
    delimiters=("```json\n", "\n```")
)

# Example Usage: Optional XML-like tag with content
# Assumes StringStateMachine is defined elsewhere
optional_xml_sm = EncapsulatedStateMachine(
    state_machine=StringStateMachine(),
    delimiters=("<data>", "</data>"),
    is_optional=True
)
```

*   **`WaitFor`**:
    *   **Purpose:** Consumes arbitrary text until a specific nested `StateMachine` is successfully triggered. Useful for skipping preamble or freeform text before structured content begins.
    *   **Use Case:** Allowing an LLM to "think" or write introductory text before starting a required structured block (like a JSON object fenced by ```json).
    *   **Example:** `WaitFor(PhraseStateMachine("```json"))` consumes all text until it encounters "```json".

```python
# Example Usage: Skip preamble until "<START>" is found
# Assumes PhraseStateMachine is defined elsewhere
wait_for_start_sm = WaitFor(
    state_machine=PhraseStateMachine("<START>")
    # buffer_length=-1 (default) means it only looks for the start phrase
)

# Example Usage: Require some buffer before looking for JSON
wait_for_json_buffered_sm = WaitFor(
    state_machine=PhraseStateMachine("```json"),
    buffer_length=20 # Requires at least 20 chars before ```json can match
)
```

## Composition

The power of these base types lies in their composition. By nesting `Chain`, `Any`, `Loop`, etc., and using `Phrase` and `Character` for the terminal elements, developers can construct arbitrarily complex `StateMachine` graphs to represent sophisticated grammars and control flows directly in Python, going beyond the capabilities of standard schema definitions when necessary. The PBA's `AgentStateMachine` is a prime example of this compositional approach.

---
pse-docs/docs/guides/framework-adapters.md
---
# Framework Adapters (Mixins)

To simplify integration with popular Python deep learning frameworks, the Proxy Structuring Engine (PSE) provides optional "mixin" classes. These mixins inherit from the framework's standard generation classes (like `transformers.GenerationMixin`) and override the necessary methods to automatically incorporate PSE's `process_logits` and `sample` hooks.

Using a mixin means you don't need to manually pass `logits_processor=[engine.process_logits]` and `sampler=engine.sample` to your `model.generate()` calls.

## Available Mixins

*   **`pse.util.torch_mixin.PSETorchMixin`**: For PyTorch models using `transformers.GenerationMixin`.
*   **`pse.util.tf_mixin.PSETFMixin`**: For TensorFlow models using `transformers.TFGenerationMixin`.
*   **`pse.util.jax_mixin.PSEFlaxMixin`**: For JAX/Flax models using `transformers.FlaxGenerationMixin`.
*   *(Note: MLX integration is often handled differently, see MLX examples)*

## How to Use (PyTorch Example)

1.  **Import:** Import the mixin alongside your base model class.
    ```python
    import torch
    from transformers import LlamaForCausalLM
    from pse.util.torch_mixin import PSETorchMixin
    ```

2.  **Create Mixed-in Class:** Define a new class that inherits from *both* the PSE mixin *and* your base model class. The order matters; typically, the mixin comes first.
    ```python
    class PSE_Llama(PSETorchMixin, LlamaForCausalLM):
        pass
    ```

3.  **Instantiate Model:** Load your model using this new combined class.
    ```python
    model = PSE_Llama.from_pretrained(...)
    ```

4.  **Attach Engine:** Create your `StructuringEngine` instance and attach it to the model using the `engine` attribute name. The mixin expects to find the engine here.
    ```python
    from pse import StructuringEngine
    # tokenizer = AutoTokenizer.from_pretrained(...)
    model.engine = StructuringEngine(tokenizer)
    ```

5.  **Configure Engine:** Configure the engine with your desired schema.
    ```python
    # from pydantic import BaseModel
    # class MySchema(BaseModel): ...
    model.engine.configure(MySchema)
    ```

6.  **Generate:** Call `model.generate()` as usual. The mixin automatically uses `model.engine.process_logits` and `model.engine.sample`.
    ```python
    # input_ids = tokenizer(...)
    output_ids = model.generate(input_ids, max_new_tokens=100, do_sample=True)
    ```

7.  **Retrieve Output:** Use the attached engine to get the structured output.
    ```python
    structured_output = model.engine.get_structured_output(MySchema)
    ```

## Benefits of Using Mixins

*   **Cleaner Code:** Avoids repetitive passing of `logits_processor` and `sampler` arguments to `generate()`.
*   **Seamless Integration:** Provides a more object-oriented way to associate the `StructuringEngine` with a specific model instance.
*   **Framework Compatibility:** Ensures PSE hooks are applied correctly within the framework's generation loop.

## Without Mixins

If you prefer not to use mixins, you can integrate PSE manually:

1.  Instantiate your base model and `StructuringEngine` separately.
    ```python
    # model = LlamaForCausalLM.from_pretrained(...)
    # engine = StructuringEngine(tokenizer)
    # engine.configure(MySchema)
    ```
2.  Pass the engine's methods directly to `generate()`:
    ```python
    output_ids = model.generate(
        input_ids,
        max_new_tokens=100,
        do_sample=True,
        logits_processor=[engine.process_logits], # Pass processor
        sampler=engine.sample                  # Pass sampler
    )
    ```
3.  Retrieve output using your separate engine instance:
    ```python
    # structured_output = engine.get_structured_output(MySchema)
    ```

While manual integration works perfectly well, the mixins offer a convenient shortcut for cleaner code, especially when working frequently with PSE and a specific model class.

---
pse-docs/docs/guides/schema-sources.md
---
# Defining Structure: Schema Sources

The Proxy Structuring Engine (PSE) is designed to be flexible, allowing you to define the desired output structure using several common and convenient methods in Python. The `StructuringEngine.configure()` method accepts various types as its `structure` argument.

## Supported Schema Sources

1.  **Pydantic Models:**
    *   **How:** Pass a Pydantic `BaseModel` class directly.
    *   **Mechanism:** PSE introspects the model's fields, types, validators, default values, and descriptions to automatically generate the corresponding JSON Schema, which is then converted into a `StateMachine`. It respects nested models, standard Python types (`str`, `int`, `list`, `dict`, etc.), `Union`, `Optional`, `Literal`, and more. Field descriptions from the model or its docstring are used.
    *   **Use Case:** Ideal when you already use Pydantic for data validation or prefer a Python-native way to define complex, typed structures. This is often the most convenient method for defining nested JSON objects.
    *   **Example:**
        ```python
        from pydantic import BaseModel

        class User(BaseModel):
            name: str
            id: int

        engine.configure(User)
        ```

2.  **JSON Schema (Dictionary):**
    *   **How:** Pass a Python dictionary representing a valid JSON Schema object.
    *   **Mechanism:** PSE directly interprets the standard JSON Schema keywords (`type`, `properties`, `items`, `required`, `enum`, `pattern`, `minLength`, `format`, `anyOf`, `$ref`, etc.) and converts them into the equivalent `StateMachine` structure. It supports local `$defs` and `$ref` for defining and reusing schema components.
    *   **Use Case:** Useful when working with existing JSON Schemas, integrating with systems that produce/consume JSON Schema, or when needing schema features not directly representable in Pydantic (though PSE's Pydantic conversion is quite comprehensive).
    *   **Example:**
        ```python
        user_schema = {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "id": {"type": "integer"}
            },
            "required": ["name", "id"]
        }
        engine.configure(user_schema)
        ```

3.  **Function Signatures (Callable):**
    *   **How:** Pass a Python function or callable.
    *   **Mechanism:** PSE introspects the function's signature (parameter names, type hints, default values) and parses its docstring (using `docstring-parser`) to generate a JSON Schema representing the function's arguments. This schema is then converted to a `StateMachine`. It aims to produce a structure suitable for generating function call arguments.
    *   **Use Case:** Convenient for generating structured arguments intended for calling a specific Python function, leveraging existing type hints and docstrings.
    *   **Example:**
        ```python
        def get_weather(city: str, unit: str = "celsius"):
            """Fetches weather for a city."""
            # ... function body ...

        engine.configure(get_weather)
        # PSE will expect JSON like: {"city": "...", "unit": "..."}
        ```

4.  **Sequence of Schemas (`list` or `tuple`):**
    *   **How:** Pass a list or tuple where each element is one of the above supported types (Pydantic model, JSON Schema dict, Callable).
    *   **Mechanism:** PSE interprets this as an "anyOf" or "oneOf" condition. It generates the schema for each item in the sequence and combines them using an `AnyStateMachine`. The LLM output must conform to *at least one* of the provided schemas.
    *   **Use Case:** When the LLM's output could validly take one of several different structures depending on the context or input.
    *   **Example:**
        ```python
        from pydantic import BaseModel

        class User(BaseModel): # ...
        class Product(BaseModel): # ...

        engine.configure([User, Product])
        # PSE expects output matching either User OR Product schema
        ```

5.  **Direct `StateMachine` Instance:**
    *   **How:** Pass an instance of a `StateMachine` class (either a base type like `ChainStateMachine` or a custom subclass).
    *   **Mechanism:** PSE uses the provided `StateMachine` directly without any conversion.
    *   **Use Case:** For advanced users who need fine-grained control over the grammar or want to implement complex, non-standard structures or control flows by composing base `StateMachine` types.
    *   **Example:**
        ```python
        from pse.types.base import ChainStateMachine, PhraseStateMachine
        custom_sm = ChainStateMachine([PhraseStateMachine("START"), PhraseStateMachine("END")])
        engine.configure(custom_sm)
        ```

By supporting these diverse sources, PSE allows developers to choose the most natural and efficient way to define their desired output structure based on their existing code, data models, or specific grammatical requirements.

---
pse-docs/docs/api/state-machine.md
---
# StateMachine API

This page details the API for the `pse_core.StateMachine` class, which is the base class for defining grammars and structures within PSE. While you often configure the `StructuringEngine` with higher-level schemas (Pydantic, JSON Schema), you can also create or subclass `StateMachine` directly for advanced control.

```python
class pse_core.StateMachine(
    state_graph: StateGraph | None = None,
    start_state: StateId = 0,
    end_states: list[StateId] | None = None,
    is_optional: bool = False,
    is_case_sensitive: bool = True,
    identifier: str | None = None
)
```

Defines a grammar as a finite state machine graph.

**Parameters:**

*   **`state_graph`** (`StateGraph | None`, optional):
    A dictionary mapping state IDs (`StateId`) to lists of transitions (`Edge`). An `Edge` is a tuple `(StateMachine, StateId)` representing a transition validated by the nested `StateMachine` leading to the target `StateId`. Defaults to an empty graph.
*   **`start_state`** (`StateId`, optional):
    The identifier of the initial state for this machine. Defaults to `0`.
*   **`end_states`** (`list[StateId] | None`, optional):
    A list of state IDs that represent valid completion points for this machine. Defaults to `["$"]`.
*   **`is_optional`** (`bool`, optional):
    If `True`, this entire state machine can be skipped during traversal. Defaults to `False`.
*   **`is_case_sensitive`** (`bool`, optional):
    If `True`, token matching within this machine (e.g., by nested `PhraseStateMachine`s) is case-sensitive. Defaults to `True`.
*   **`identifier`** (`str | None`, optional):
    A human-readable identifier for this state machine instance, useful for debugging and stateful output retrieval. Defaults to `None`.

---

## Methods

### `get_new_stepper`

```python
get_new_stepper(
    state: StateId | None = None
) -> Stepper
```

Creates a new `Stepper` instance initialized to traverse this `StateMachine`.

**Parameters:**

*   **`state`** (`StateId | None`, optional):
    The state ID to start the stepper at. If `None`, uses the machine's `start_state`. Defaults to `None`.

**Returns:**

*   (`Stepper`): A new stepper instance.

---

### `get_steppers`

```python
get_steppers(
    state: StateId | None = None
) -> list[Stepper]
```

Creates one or more initial `Stepper` instances for this machine. Can return multiple steppers if the starting state allows for different initial paths (e.g., via an `AnyStateMachine` transition).

**Parameters:**

*   **`state`** (`StateId | None`, optional):
    The state ID to start the steppers at. If `None`, uses the machine's `start_state`. Defaults to `None`.

**Returns:**

*   (`list[Stepper]`): A list of new stepper instances.

---

### `get_edges`

```python
get_edges(
    state: StateId
) -> list[Edge]
```

Returns the list of outgoing transitions (`Edge` tuples) from the specified state.

**Parameters:**

*   **`state`** (`StateId`): The state ID to query.

**Returns:**

*   (`list[Edge]`): A list of `(StateMachine, StateId)` tuples representing valid transitions.

---

### `get_transitions`

```python
get_transitions(
    stepper: Stepper
) -> list[tuple[Stepper, StateId]]
```

Gets the possible initial sub-steppers and target states for transitions originating from the given stepper's current state.

**Parameters:**

*   **`stepper`** (`Stepper`): The stepper whose current state's transitions are being queried.

**Returns:**

*   (`list[tuple[Stepper, StateId]]`): A list where each tuple contains a new sub-stepper (for validating the transition) and the target state ID in the parent machine.

---

### `advance_stepper`

```python
advance_stepper(
    stepper: Stepper,
    token: str
) -> list[Stepper]
```

Advances a given stepper by consuming a token string. This is primarily used internally by `Stepper.consume`.

**Parameters:**

*   **`stepper`** (`Stepper`): The stepper to advance.
*   **`token`** (`str`): The token string to consume.

**Returns:**

*   (`list[Stepper]`): A list of new steppers representing the possible states after consuming the token.

---

### `branch_stepper`

```python
branch_stepper(
    stepper: Stepper,
    token: str | None = None
) -> list[Stepper]
```

Creates multiple branched steppers from a given stepper, exploring different possible transitions simultaneously. Used internally by `Stepper.branch`.

**Parameters:**

*   **`stepper`** (`Stepper`): The stepper to branch from.
*   **`token`** (`str | None`, optional): An optional token to consider when determining valid branches. Defaults to `None`.

**Returns:**

*   (`list[Stepper]`): A list of new steppers representing different branches.

---

### `advance_all` (Static Method)

```python
@staticmethod
advance_all(
    steppers: list[Stepper],
    token: str,
    vocab: TrieMap | None = None,
    token_healing: bool = True
) -> list[StepperDelta]
```

Advances multiple steppers simultaneously with a single token. Handles token healing if `vocab` is provided. This is primarily used internally by the `StructuringEngine`.

**Parameters:**

*   **`steppers`** (`list[Stepper]`): The list of active steppers to advance.
*   **`token`** (`str`): The token string to consume.
*   **`vocab`** (`TrieMap | None`, optional): The vocabulary TrieMap for token healing. Defaults to `None`.
*   **`token_healing`** (`bool`, optional): Whether to enable token healing. Defaults to `True`.

**Returns:**

*   (`list[StepperDelta]`): A list of `StepperDelta` objects representing the results of the transitions for each input stepper.

---

### `advance_all_basic` (Static Method)

```python
@staticmethod
advance_all_basic(
    steppers: list[Stepper],
    token: str
) -> list[Stepper]
```

A simplified version of `advance_all` that only returns the resulting steppers, discarding metadata like healing status.

**Parameters:**

*   **`steppers`** (`list[Stepper]`): The list of active steppers to advance.
*   **`token`** (`str`): The token string to consume.

**Returns:**

*   (`list[Stepper]`): A list of the resulting steppers after consuming the token.

---

## Properties

*   **`state_graph`** (`StateGraph`):
    The dictionary defining the state transitions. Read/write.
*   **`start_state`** (`StateId`):
    The identifier of the starting state. Read/write.
*   **`end_states`** (`list[StateId]`):
    The list of identifiers for valid end states. Read/write.
*   **`is_optional`** (`bool`):
    Whether the state machine is optional. Read/write.
*   **`is_case_sensitive`** (`bool`):
    Whether token matching should be case-sensitive. Read/write.
*   **`identifier`** (`str | None`):
    The human-readable identifier for this instance. Read/write.

---
pse-docs/docs/api/stepper.md
---
# Stepper API

This page details the API for the `pse_core.Stepper` class. Steppers track the current position within a `StateMachine` during generation. Developers typically don't instantiate `Stepper`s directly but interact with them via the `StructuringEngine`. Understanding the `Stepper` API can be helpful for debugging or advanced customization.

```python
class pse_core.Stepper(
    state_machine: StateMachine,
    current_state: StateId | None = None
)
```

Represents a position within a state machine and manages traversal.

**Parameters:**

*   **`state_machine`** (`StateMachine`):
    The `StateMachine` instance this stepper will traverse.
*   **`current_state`** (`StateId | None`, optional):
    The initial state ID within the `state_machine`. If `None`, defaults to the `state_machine`'s `start_state`.

---

## Methods

### `clone`

```python
clone() -> Stepper
```

Creates a deep copy of this stepper, including its current state, history, and associated sub-stepper (if any). Essential for exploring multiple grammar paths.

**Returns:**

*   (`Stepper`): A new `Stepper` instance identical to the original.

---

### `consume`

```python
consume(
    token: str
) -> list[Stepper]
```

Consumes a token string and advances the stepper's state according to its `StateMachine`. This is the primary method for processing tokens during generation, often called internally by the `StructuringEngine`. It handles transitions, sub-stepper management, and branching.

**Parameters:**

*   **`token`** (`str`): The token string to consume.

**Returns:**

*   (`list[Stepper]`): A list of new `Stepper` instances representing all possible valid states after consuming the token. Returns an empty list if the token leads to no valid states.

---

### `get_current_value`

```python
get_current_value() -> Any
```

Returns the accumulated value parsed from the raw string generated along the stepper's path so far. Attempts to parse as JSON (number, boolean, object, array) first, falling back to the raw string if JSON parsing fails.

**Returns:**

*   (`Any`): The parsed Python object (e.g., `int`, `float`, `bool`, `dict`, `list`) or the raw `str`. Returns `None` if no value has been accumulated.

---

### `get_raw_value`

```python
get_raw_value() -> str
```

Returns the raw, concatenated string output generated along the stepper's path (including history and any active sub-stepper) without attempting any parsing or type conversion.

**Returns:**

*   (`str`): The raw accumulated string.

---

### `get_valid_continuations`

```python
get_valid_continuations() -> list[str]
```

Returns a list of strings representing all valid token sequences that can legally follow the stepper's current state according to its `StateMachine`. Used by the `StructuringEngine` to determine which tokens to allow during logit processing.

**Returns:**

*   (`list[str]`): A list of valid continuation strings.

---

### `get_invalid_continuations`

```python
get_invalid_continuations() -> list[str]
```

Returns a list of strings that are explicitly *invalid* continuations from the current state. This is less commonly used than `get_valid_continuations` but can be implemented by custom `StateMachine` subclasses for specific exclusion rules.

**Returns:**

*   (`list[str]`): A list of invalid continuation strings.

---

### `has_reached_accept_state`

```python
has_reached_accept_state() -> bool
```

Checks if the stepper (and its sub-stepper, if active) is currently in a state designated as an `end_state` by its `StateMachine`.

**Returns:**

*   (`bool`): `True` if the stepper is in a valid terminal state, `False` otherwise.

---

### `can_accept_more_input`

```python
can_accept_more_input() -> bool
```

Checks if the stepper can consume more tokens based on the rules of its current `StateMachine` (e.g., character limits in a `CharacterStateMachine`).

**Returns:**

*   (`bool`): `True` if more input can be processed, `False` otherwise.

---

### `is_within_value`

```python
is_within_value() -> bool
```

Indicates if the stepper is currently in the process of accumulating characters for a specific value (e.g., inside a string literal, number, etc.), as opposed to being between structural elements.

**Returns:**

*   (`bool`): `True` if actively consuming value characters.

---

### `accepts_any_token`

```python
accepts_any_token() -> bool
```

Indicates if the stepper's current state allows *any* token as a valid continuation (often true for free-form text states).

**Returns:**

*   (`bool`): `True` if any token is currently valid.

---

### `get_identifier`

```python
get_identifier() -> str | None
```

Returns the identifier string associated with the stepper's current `StateMachine` or its active sub-stepper's `StateMachine`. Used by `get_stateful_structured_output`.

**Returns:**

*   (`str | None`): The identifier string, or `None`.

---

### `get_token_ids_history`

```python
get_token_ids_history() -> list[int]
```

Returns the sequence of token IDs that were consumed along the path taken by this stepper and its history. Used for accurate output reconstruction via `get_token_safe_output`.

**Returns:**

*   (`list[int]`): The list of consumed token IDs.

---

### `get_token_safe_output`

```python
get_token_safe_output(
    decode_function: Callable[[list[int]], str]
) -> str
```

Reconstructs the generated string output accurately by decoding the stored `token_ids_history` using the provided `decode_function`. This avoids potential errors from decoding the `raw_value` directly, especially if token healing occurred.

**Parameters:**

*   **`decode_function`** (`Callable[[list[int]], str]`): The tokenizer's decode function.

**Returns:**

*   (`str`): The accurately reconstructed output string.

---

## Properties

*   **`state_machine`** (`StateMachine`):
    The `StateMachine` this stepper is traversing. Read/write.
*   **`current_state`** (`StateId`):
    The current state ID within the `state_machine`. Read/write.
*   **`target_state`** (`StateId | None`):
    The target state ID for an in-progress transition. Read/write.
*   **`sub_stepper`** (`Stepper | None`):
    The active sub-stepper handling a nested `StateMachine` traversal, if any. Read/write.
*   **`history`** (`list[Stepper]`):
    The list of completed sub-steppers that led to the current state. Read/write.
*   **`consumed_character_count`** (`int`):
    The number of characters consumed along this stepper's path. Read/write.
*   **`remaining_input`** (`str | None`):
    Any portion of the last consumed token that was *not* processed by the current step. Read/write.
*   **`_raw_value`** (`str | None`):
    The raw accumulated string value. Use `get_raw_value()` for access. Read/write (internal use primarily).

---
pse-docs/docs/api/structuring-engine.md
---
# Structuring Engine API

This page details the API for the main `pse.StructuringEngine` class, which is the primary interface for using the Proxy Structuring Engine.

```python
class pse.StructuringEngine(
    tokenizer: PreTrainedTokenizerFast | PreTrainedTokenizerBase,
    whitelist_control_tokens: list[str] | None = None,
    multi_token_sampling: bool = False,
    max_resample_attempts: int = 5
)
```

Inherits from the core C++ `Engine`. Orchestrates token processing and interfaces with language models.

**Parameters:**

*   **`tokenizer`** (`PreTrainedTokenizerFast | PreTrainedTokenizerBase`):
    An initialized tokenizer instance from the Hugging Face `transformers` library. Used to access the vocabulary, encode text to token IDs, and decode token IDs to text.
*   **`whitelist_control_tokens`** (`list[str] | None`, optional):
    A list of control token strings (e.g., `"<|eot_id|>"`) that should *not* be automatically masked by the engine, even if they might otherwise be considered invalid by the grammar near the end of generation. This prevents the engine from blocking essential control tokens like EOS. Defaults to `None`.
*   **`multi_token_sampling`** (`bool`, optional):
    Enables or disables the Multi-Token Processing optimization. When `True` (default in source, but shown as `False` here - *Correction: Default is `False` in provided `__init__` signature*), allows the engine to potentially return multiple tokens at once if an unambiguous multi-token sequence is required by the grammar. Defaults to `False`.
*   **`max_resample_attempts`** (`int`, optional):
    The maximum number of times the engine will ask the base sampler for a new token if the initially sampled token is invalid according to the grammar. Helps find a valid token when the probability mass is concentrated on invalid options. Defaults to `5`.

---

## Methods

### `configure`

```python
configure(
    structure: JSONSchemaSource | StateMachine,
    **kwargs: Any
) -> None
```

Configures the engine with the desired output structure. This translates the provided schema into the internal `StateMachine` representation used for enforcement.

**Parameters:**

*   **`structure`** (`JSONSchemaSource | StateMachine`):
    The schema definition. Can be a Pydantic `BaseModel` class, a JSON Schema dictionary, a Python callable (function signature), a sequence (`list` or `tuple`) of these types (interpreted as `anyOf`), or a direct `StateMachine` instance. See [Schema Sources Guide](../guides/schema-sources.md).
*   **`**kwargs`**:
    Additional keyword arguments passed to the schema conversion process (e.g., `delimiters` or `buffer_length` when using `EncapsulatedStateMachine` or `WaitFor` implicitly or explicitly).

---

### `process_logits`

```python
process_logits(
    input_ids: Any, # Framework-specific tensor/array
    scores: Any     # Framework-specific tensor/array
) -> Any          # Framework-specific tensor/array
```

The primary logits processing hook. This method should be added to the `logits_processor` list in your generation call. It queries the internal `Stepper`(s) to determine valid next tokens based on the `StateMachine` and masks the `scores` (logits) tensor, setting invalid token probabilities to negative infinity.

**Parameters:**

*   **`input_ids`**: The input token IDs tensor/array provided by the generation framework.
*   **`scores`**: The logits tensor/array (typically shape `(batch_size, vocab_size)`) produced by the LLM for the current step.

**Returns:**

*   The modified logits tensor/array with invalid tokens masked.

---

### `sample`

```python
sample(
    logprobs: Any, # Framework-specific tensor/array
    sampler: Callable[..., Any]
) -> Any       # Framework-specific tensor/array
```

The sampling hook. This method should be used as the `sampler` function in your generation call. It takes the processed logits (`logprobs`), calls the provided base `sampler` function, checks the validity of the sampled token(s), potentially resamples up to `max_resample_attempts`, handles multi-token processing, advances the internal `Stepper`(s), and returns the final chosen token ID(s).

**Parameters:**

*   **`logprobs`**: The processed logits tensor/array (shape `(batch_size, vocab_size)`) after `process_logits` has been applied.
*   **`sampler`**: The base sampling function from your framework (e.g., `torch.multinomial`, `jax.random.categorical`, `tf.random.stateless_categorical`) which takes the logits and returns sampled token ID(s).

**Returns:**

*   A tensor/array containing the chosen token ID(s) for the current step.

---

### `get_structured_output`

```python
get_structured_output(
    output_type: type[OutputType] | None = None,
    raise_on_error: bool = False
) -> OutputType | Any
```

Retrieves the final generated output string from the engine's state, parses it (primarily as JSON), and optionally validates/casts it to a specified Python type (like a Pydantic model).

**Parameters:**

*   **`output_type`** (`type[OutputType] | None`, optional):
    The target Python type (e.g., a Pydantic `BaseModel` subclass) to parse and validate the output against. If `None`, the raw parsed JSON object (or string if JSON parsing fails) is returned. Defaults to `None`.
*   **`raise_on_error`** (`bool`, optional):
    If `True`, raises an error if JSON parsing or Pydantic validation fails. If `False`, logs the error and returns the raw string or partially parsed object. Defaults to `False`.

**Returns:**

*   An instance of `output_type` if provided and validation succeeds, otherwise the parsed JSON object, or the raw string if parsing fails and `raise_on_error` is `False`.

---

### `get_stateful_structured_output`

```python
get_stateful_structured_output(
    output_type: type[OutputType] | None = None,
    raise_on_error: bool = False
) -> Iterator[tuple[str, OutputType | Any]]
```

Retrieves the generated output segmented by the state identifier that produced each part. Useful for complex state machines (like in PBA) where different parts of the output correspond to different logical steps (e.g., "thinking", "tool_call").

**Parameters:**

*   **`output_type`** (`type[OutputType] | None`, optional):
    The target Python type to parse/validate each segment against (applied individually). Defaults to `None`.
*   **`raise_on_error`** (`bool`, optional):
    Whether to raise errors during parsing/validation of segments. Defaults to `False`.

**Returns:**

*   An iterator yielding tuples of `(state_identifier: str, parsed_output: OutputType | Any)`.

---

### `get_live_structured_output`

```python
get_live_structured_output() -> tuple[str, str] | None
```

Attempts to retrieve the *current*, potentially *incomplete* output being generated, along with the identifier of the state currently being processed. Useful for streaming or live display. Relies on `Stepper.get_token_safe_output`.

**Returns:**

*   A tuple `(state_identifier, current_output_string)` if available, otherwise `None`.

---

### `reset`

```python
reset(
    hard_reset: bool = False
) -> None
```

Resets the engine's internal state, clearing active `Stepper`s. If `hard_reset` is `True`, it also removes the configured `StateMachine`.

**Parameters:**

*   **`hard_reset`** (`bool`, optional): If `True`, removes the configured `StateMachine` in addition to resetting steppers. Defaults to `False`.

---

## Properties

*   **`has_reached_accept_state`** (`bool`, read-only):
    Returns `True` if any of the active `Stepper`s have reached a valid end state according to the configured `StateMachine`.
*   **`state_machine`** (`StateMachine | None`):
    The currently configured root `StateMachine` instance. Can be set directly or via `configure`.
*   **`steppers`** (`list[Stepper]`):
    The list of currently active `Stepper` objects representing the engine's state within the `StateMachine`. Can be read or set directly (advanced use).
*   **`vocabulary`** (`TrieMap`, read-only):
    The vocabulary map (string to token ID list) used by the engine, derived from the tokenizer.
*   **`reverse_vocabulary`** (`dict[int, str]`, read-only):
    The reverse vocabulary map (token ID to string).
*   **`multi_token_mapping`** (`dict[int, list[int]]`):
    The internal mapping used for multi-token processing. Can be read or set (advanced use).

---
pse-docs/docs/api/state-machines/base.md
---
# Base State Machines

Base state machines provide the fundamental building blocks for constructing more complex state machines in PSE. These components handle common pattern-matching tasks and can be composed together to create sophisticated parsing behaviors.

## AnyStateMachine

`AnyStateMachine` allows choosing between multiple state machines, effectively implementing a logical OR operation for state machine matching.

```python
from pse.types.base.any import AnyStateMachine
from pse.types.string import StringStateMachine
from pse.types.number import NumberStateMachine

# Create a state machine that accepts either a string or a number
any_state_machine = AnyStateMachine([
    StringStateMachine(),
    NumberStateMachine()
])
```

**Key features:**
- Accepts input if any of its child state machines accepts it
- Creates transitions from the initial state to each alternative state machine
- Collects steppers from all possible edges to enable parallel path exploration
- Essential for implementing union types, optional patterns, and alternatives
- Lightweight implementation with no additional parameters beyond the list of state machines

**Implementation details:**
- Creates a state graph where state 0 transitions directly to the end state ("$") through each provided state machine
- Uses a simple graph structure with a single state (0) and multiple transitions
- On input, explores all possible paths in parallel through its steppers
- Returns a simple string representation of "Any" for debugging purposes

## ChainStateMachine

`ChainStateMachine` combines multiple state machines in sequence, requiring each to match in order.

```python
from pse.types.base.chain import ChainStateMachine
from pse.types.base.phrase import PhraseStateMachine
from pse.types.string import StringStateMachine
from pse.types.whitespace import WhitespaceStateMachine

# Create a state machine for parsing a key-value pair like "name: "John""
chain_state_machine = ChainStateMachine([
    StringStateMachine(),  # The key
    WhitespaceStateMachine(),  # Optional whitespace
    PhraseStateMachine(":"),  # The separator
    WhitespaceStateMachine(),  # Optional whitespace
    StringStateMachine()  # The value
])

# Create an optional chain
optional_chain = ChainStateMachine([
    PhraseStateMachine(","),
    WhitespaceStateMachine()
], is_optional=True)
```

**Key features:**
- Enforces sequential matching of multiple patterns
- Propagates state from one sub-machine to the next
- Supports optional chains with the `is_optional` parameter
- Essential for constructing complex sequential patterns
- Used extensively in format-specific state machines

**Implementation details:**
- Creates a linear state graph where each state `i` transitions to state `i+1` through its corresponding state machine
- Automatically sets the final state `len(state_machines)` as the only end state
- Uses a specialized `ChainStepper` class to handle sequential execution
- Minimal implementation that efficiently chains state machines together
- Accurately tracks state transitions through the sequence
- Handles nested chains and complex sequences within larger state machines

## LoopStateMachine

`LoopStateMachine` repeats a state machine a configurable number of times, enabling iteration patterns.

```python
from pse.types.base.loop import LoopStateMachine
from pse.types.number import NumberStateMachine
from pse.types.whitespace import WhitespaceStateMachine

# Create a state machine for a comma-separated list of numbers
numbers_list = LoopStateMachine(
    NumberStateMachine(),
    min_loop_count=1,  # At least one number
    max_loop_count=5,  # Maximum five numbers
    whitespace_seperator=True  # Allow whitespace between elements
)
```

**Key features:**
- Configurable minimum and maximum repetition counts
- Optional whitespace handling between iterations
- Maintains loop count tracking through the specialized `LoopStepper` class
- Implements arrays, lists, and repeated patterns
- Supports unlimited repetition with `max_loop_count=-1` (default)

**Implementation details:**
- Creates two different state graph structures based on `whitespace_seperator`:
  - Without separator: State 0 â†’ State 1 (via state_machine) â†’ back to State 0 (via state_machine again)
  - With separator: State 0 â†’ State 1 (via state_machine) â†’ State 2 (via whitespace) â†’ back to State 1 (via state_machine)
- Uses the `is_optional` parameter internally calculated from `min_loop_count == 0`
- Contains a typo in parameter name ("whitespace_seperator" instead of "separator")
- Specialized `LoopStepper` tracks iteration count with `loop_count` property, incrementing in `add_to_history`
- Whitespace separators are excluded from history and loop count calculation
- Determines acceptance state based on the minimum loop count and sub-stepper state
- Enforces maximum iterations by overriding `can_accept_more_input` and `should_start_step`
- Provides special handling of final state to exclude whitespace separators

## PhraseStateMachine

`PhraseStateMachine` matches specific text sequences exactly, validating input against the target string.

```python
from pse.types.base.phrase import PhraseStateMachine

# Create a state machine that matches only the text "Hello, World!"
greeting = PhraseStateMachine("Hello, World!")

# Create an optional phrase that may or may not appear
optional_phrase = PhraseStateMachine("Optional text", is_optional=True)

# Create a case-insensitive phrase
case_insensitive = PhraseStateMachine("HELLO", is_case_sensitive=False)
```

**Key features:**
- Matches exact character sequences with character-by-character validation
- High-efficiency implementation for literal text using optimized prefix matching
- Support for Unicode characters and multi-byte sequences
- Case sensitivity configuration with the `is_case_sensitive` parameter
- Can be marked as optional with the `is_optional` parameter
- Used for keywords, delimiters, and fixed tokens throughout PSE

**Implementation details:**
- Requires a non-empty phrase string (raises `ValueError` if empty)
- Uses a specialized `PhraseStepper` that tracks position with `consumed_character_count`
- Sets target state directly to end state ("$") in the stepper
- Implements prefix matching with the optimized `_get_valid_match_length` method
- Processes partial matches by accepting as much of the input as matches the phrase
- Returns the exact remaining portion of the phrase for valid continuations
- String representation includes the phrase value: `Phrase('value')`
- Completion is determined when `consumed_character_count == len(phrase)`
- Can check for equality with other PhraseStateMachine instances
- Essential building block for many other state machines in PSE

## WaitFor

`WaitFor` accumulates arbitrary text until a specific pattern is detected, enabling flexible pattern recognition in mixed content.

```python
from pse.types.base.wait_for import WaitFor
from pse.types.base.phrase import PhraseStateMachine

# Create a state machine that accumulates text until "```json" is encountered
# Useful for finding JSON code blocks in Markdown
json_block_start = WaitFor(
    PhraseStateMachine("```json"),
    buffer_length=0  # No minimum buffer required
)

# Create a state machine with strict validation
# If the pattern starts but is interrupted, the match will fail
strict_pattern = WaitFor(
    PhraseStateMachine("BEGIN DATA"),
    buffer_length=10,  # Require at least 10 characters before matching pattern
    strict=True  # Fail if pattern is interrupted
)
```

**Key features:**
- Buffers text until target pattern is detected
- Configurable minimum buffer length with `buffer_length` parameter
- Strict mode for handling invalid inputs with the `strict` parameter
- Perfect for delimiter-based parsing of mixed content
- Enables flexible handling of unstructured text before structured elements

**Implementation details:**
- Uses a direct transition structure with the target pattern transitioning to end state ("$")
- `get_transitions` creates transitions directly from the nested state machine's steppers
- Specialized `WaitForStepper` maintains accumulated text in a `buffer` property
- Sets `target_state = "$"` in the stepper initialization
- Implements `accepts_any_token()` based on buffer length and sub-stepper state
- `should_start_step()` checks:
  - Buffer length requirements
  - Presence of remaining input
  - Current processing state
- When `strict=True`, rejects entire input if the pattern is interrupted
- When `strict=False`, allows pattern matching to restart after interruption
- With `buffer_length=-1`, only accepts tokens that match the target pattern
- With `buffer_length=0`, accepts any token while watching for the pattern
- With `buffer_length>0`, requires buffer to reach minimum length before allowing match
- Provides specialized handling of valid and invalid continuations
- Efficiently processes tokens by finding valid prefixes using token partitioning

## CharacterStateMachine

`CharacterStateMachine` matches individual characters based on character sets, allowing precise control over which characters are accepted.

```python
from pse.types.base.character import CharacterStateMachine

# Create a state machine that matches any character (no restrictions)
any_char_sm = CharacterStateMachine()

# Create a state machine that matches only lowercase letters a-z
alpha_sm = CharacterStateMachine(whitelist_charset="abcdefghijklmnopqrstuvwxyz")

# Create a state machine that matches any character except digits
no_digits_sm = CharacterStateMachine(blacklist_charset="0123456789")

# Create a state machine with multiple constraints
identifier_sm = CharacterStateMachine(
    whitelist_charset="abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_",
    char_min=1,          # At least one character
    char_limit=64,       # Maximum of 64 characters
    case_sensitive=True  # Case-sensitive matching
)

# Use graylist for characters that terminate a match
word_sm = CharacterStateMachine(
    graylist_charset=" \t\n.,:;!?"  # Space and punctuation terminate words
)
```

**Key features:**
- Highly flexible character-level matching with configurable character sets
- Supports whitelist (explicitly allowed), blacklist (explicitly forbidden), and graylist (allowed but terminates sequence) character sets
- Configurable minimum and maximum character counts
- Case-sensitive or case-insensitive matching
- Efficiently processes longest valid prefixes in input tokens

**Implementation details:**
- Configurable parameters with clear defaults:
  - Empty charsets by default (empty strings)
  - `char_min` defaults to 0 (no minimum length)
  - `char_limit` defaults to 0 (unlimited length)
  - `is_optional` defaults to false
  - `case_sensitive` defaults to true
- Three distinct character set types stored as `set` objects:
  - `charset` (whitelist): Only these characters are allowed
  - `blacklist_charset`: These characters are explicitly forbidden
  - `graylist_charset`: Allowed but terminate matching when after other characters
- Case-insensitive mode converts all characters to lowercase
- `CharacterStepper` sets `target_state="$"` and tracks value with `_raw_value`
- Implements `accepts_any_token()` to return true when no whitelist is defined
- Uses `should_start_step()` to validate the first character against constraints
- Implements efficient prefix matching in `consume()`:
  - Checks each character in token for validity against all constraints
  - Finds longest valid prefix that satisfies all character constraints
  - Efficiently handles blacklist, whitelist, length limits, and graylist checks
  - Preserves remaining input for continued processing
- Returns the complete whitelist charset for valid continuations
- Allows early termination of matching when graylist characters appear

## EncapsulatedStateMachine

`EncapsulatedStateMachine` wraps a state machine between start and end delimiters, enabling parsing of fenced or delimited content.

```python
from pse.types.base.encapsulated import EncapsulatedStateMachine
from pse.types.base.any import AnyStateMachine
from pse.types.string import StringStateMachine
from pse.types.number import NumberStateMachine

# Create a state machine for parsing JSON code blocks in Markdown
json_code_block = EncapsulatedStateMachine(
    ObjectStateMachine(),              # Content state machine for JSON objects
    delimiters=("```json\n", "\n```"),  # Start and end delimiters
    buffer_length=0                     # No minimum buffer before delimiter
)

# Create a state machine for XML tags with content
xml_content = EncapsulatedStateMachine(
    StringStateMachine(),              # Content for the XML element
    delimiters=("<content>", "</content>")  # XML tags
)

# Create a state machine that requires minimum buffer before finding delimiter
delayed_content = EncapsulatedStateMachine(
    StringStateMachine(),              # Content state machine
    delimiters=("<START>", "<END>"),   # Delimiters
    buffer_length=10,                  # Require at least 10 characters before matching delimiter
    is_optional=True                   # Make the entire structure optional
)
```

**Key features:**
- Wraps any state machine between specified start and end delimiters
- Configurable buffer length before matching start delimiter
- Can be optional with the `is_optional` parameter
- Perfect for parsing code blocks, markup elements, and delimited content
- Provides clean token output with delimiters automatically removed

**Implementation details:**
- Implements a three-state process with a clear state graph:
  - State 0: Uses `WaitFor` with `PhraseStateMachine` to find the start delimiter
  - State 1: Processes the inner content using the provided state machine
  - State 2: Matches the end delimiter using a `PhraseStateMachine`
- Default delimiters are ("```", "```") if none are provided
- `buffer_length` parameter is passed to the internal `WaitFor` state machine
- Uses specialized `EncapsulatedStepper` to track state and content
- Maintains an `inner_stepper` reference for the state machine's stepper in state 2
- `is_within_value()` returns true when not in state 0 or when in state 0 with an active sub-stepper
- Provides `get_invalid_continuations()` to prevent end delimiter before processing starts
- Implements the `get_token_safe_output()` helper method which:
  - Efficiently handles both exact and partial delimiter occurrences
  - Optimizes removal by checking for exact matches before using strip operations
  - Processes token IDs history for reliable delimiter removal
- Can handle nested encapsulated content with proper tracking
- Used extensively for parsing structured content like code blocks and XML elements


---
pse-docs/docs/api/state-machines/datatypes.md
---
# Data Type State Machines

PSE provides specialized state machines for handling common data types, allowing precise control over the format and content of generated values.

## StringStateMachine

`StringStateMachine` handles JSON-style string parsing with double quotation marks and proper escaping of special characters.

```python
from pse.types.string import StringStateMachine

# Create a basic string state machine that matches quoted strings like "hello"
string_sm = StringStateMachine()

# Create a string state machine with length constraints
constrained_string_sm = StringStateMachine(
    min_length=3,  # Minimum string length (excluding quotes)
    max_length=50  # Maximum string length (excluding quotes)
)
```

**Implementation details:**
- State machine structure:
  - Initial state (0): Expects opening double quote (")
  - String contents state (1): Processes regular characters
  - Escape sequence state (2): Handles escape sequences
  - Unicode escape state (3): Processes \uXXXX Unicode escapes

- Escape sequence handling:
  - Standard JSON escape sequences: \", \\, \/, \b, \f, \n, \r, \t
  - Unicode escape sequences: \uXXXX (4 hex digits)
  - Automatically rejects invalid escape sequences

- Character handling:
  - Properly validates all string content
  - Automatically rejects control characters that must be escaped
  - Maintains proper JSON string compliance

**StringSchemaStateMachine extension**:
For JSON Schema integration, the StringSchemaStateMachine adds pattern and format validation:

```python
from pse.types.json.json_string import StringSchemaStateMachine

# Create a string state machine with JSON Schema constraints
email_string_sm = StringSchemaStateMachine(
    schema={"type": "string", "format": "email"},
    context={}
)

# Create a string state machine with pattern validation
pattern_string_sm = StringSchemaStateMachine(
    schema={"type": "string", "pattern": "^[A-Z]{2}-\\d{4}$"},  # Format like "AB-1234"
    context={}
)
```

**Example: String with escape sequences**
```python
from pse.types.string import StringStateMachine

# Define a string state machine
string_sm = StringStateMachine()

# This will match: "Hello\nWorld" and correctly handle the newline
# The value returned will be the string without quotes: Hello\nWorld
```

**Example: Unicode handling**
```python
from pse.types.string import StringStateMachine

# Define a string state machine
string_sm = StringStateMachine()

# This will match: "Hello\u0057orld" and correctly interpret it as "HelloWorld"
# This also matches: "Hello ä¸–ç•Œ" as Unicode is supported via direct inclusion or escapes
```

**Example: Length validation**
```python
from pse.types.string import StringStateMachine

# Create a string state machine with length constraints
username_sm = StringStateMachine(min_length=3, max_length=20)

# This will match: "user123"
# This will not match: "a" (too short)
# This will not match: "this_username_is_way_too_long" (too long)
```

## NumberStateMachine

`NumberStateMachine` parses JSON-compliant numeric values, handling integers, decimals, and scientific notation.

```python
from pse.types.number import NumberStateMachine

# Create a basic number state machine for all numeric formats
number_sm = NumberStateMachine()
```

**Implementation details:**
- State machine structure:
  - State 0: Starting state that optionally accepts a negative sign (-)
  - State 1: Expects an integer part (mandatory)
  - State 2: Accept state for integers, can also transition to decimal part
  - State 3: Decimal part state (after the decimal point), also an accept state
  - State 4: Exponential notation marker 'e' or 'E'
  - State 5: Optional sign for the exponent (+ or -)
  - Final state: Exponent integer value, if present

- Decimal handling:
  - Properly handles decimal points followed by integer part
  - Maintains leading zeros after decimal point (e.g., 0.001)
  - Validates full decimal number structure

- Scientific notation:
  - Supports 'e' or 'E' notation (e.g., 1.5e3, 2E-4)
  - Handles optional sign in exponent
  - Properly combines base number with exponent for final value

**NumberSchemaStateMachine extension**:
For JSON Schema integration, use the NumberSchemaStateMachine to add validation constraints:

```python
from pse.types.json.json_number import NumberSchemaStateMachine

# Create a number state machine with JSON Schema constraints
constrained_number_sm = NumberSchemaStateMachine(
    schema={
        "type": "number",
        "minimum": 0,
        "maximum": 100,
        "exclusiveMaximum": True
    },
    context={}
)

# Create an integer-only state machine
integer_number_sm = NumberSchemaStateMachine(
    schema={
        "type": "integer",
        "multipleOf": 5
    },
    context={}
)
```

**Example: Basic number parsing**
```python
from pse.types.number import NumberStateMachine

# Define a number state machine
number_sm = NumberStateMachine()

# This will match: "42" (integer)
# This will match: "-3.14" (negative decimal)
# This will match: "1.618e2" (scientific notation, equal to 161.8)
```

**Example: JSON Schema validation**
```python
from pse.types.json.json_number import NumberSchemaStateMachine

# Define a number state machine with constraints
temperature_sm = NumberSchemaStateMachine(
    schema={
        "type": "number",
        "minimum": -273.15,  # Absolute zero in Celsius
        "maximum": 1000
    },
    context={}
)

# This will match: "25.5" (valid temperature)
# This will not match: "-300" (below absolute zero)
# This will not match: "1200" (above maximum)
```

**Example: Integer validation**
```python
from pse.types.json.json_number import NumberSchemaStateMachine

# Define an integer state machine with constraints
page_number_sm = NumberSchemaStateMachine(
    schema={
        "type": "integer",
        "minimum": 1
    },
    context={}
)

# This will match: "42" (valid page number)
# This will not match: "2.5" (not an integer)
# This will not match: "0" (below minimum)
```

## IntegerStateMachine

`IntegerStateMachine` is a specialized state machine for parsing positive integer values with configurable handling of leading zeros.

```python
from pse.types.integer import IntegerStateMachine

# Create a basic integer state machine
int_sm = IntegerStateMachine()

# Create an integer state machine that preserves leading zeros
raw_int_sm = IntegerStateMachine(drop_leading_zeros=False)
```

**Implementation details:**
- Extends CharacterStateMachine with digit characters (0-9)
- Only accepts positive integers (no negative sign support)
- By default, removes leading zeros and returns an int value
- When drop_leading_zeros=False, preserves the original string representation

**Example: Basic integer parsing**
```python
from pse.types.integer import IntegerStateMachine

# Define an integer state machine
int_sm = IntegerStateMachine()

# This will match: "42" -> 42 (int)
# This will match: "007" -> 7 (int, leading zeros removed)
# This will not match: "-5" (negative not supported)
# This will not match: "3.14" (decimals not supported)
```

**Example: Preserving leading zeros**
```python
from pse.types.integer import IntegerStateMachine

# Create an integer state machine that preserves leading zeros
raw_int_sm = IntegerStateMachine(drop_leading_zeros=False)

# This will match: "42" -> "42" (string)
# This will match: "007" -> "007" (string, leading zeros preserved)
# This will not match: "-5" (negative not supported)
# This will not match: "3.14" (decimals not supported)
```

**Note:** For more complex integer requirements (negative numbers, range validation), use `NumberSchemaStateMachine` with a JSON Schema:

```python
from pse.types.json.json_number import NumberSchemaStateMachine

# Create a state machine for integers with validation constraints
integer_sm = NumberSchemaStateMachine(
    schema={
        "type": "integer",
        "minimum": 1,
        "maximum": 1000
    },
    context={}
)
```

## BooleanStateMachine

`BooleanStateMachine` parses JSON-compliant boolean literals ("true"/"false").

```python
from pse.types.boolean import BooleanStateMachine

# Create a boolean state machine
bool_sm = BooleanStateMachine()
```

**Implementation details:**
- State machine structure:
  - Initial state (0): Expects exact match of "true" or "false" literals
  - Uses PhraseStateMachine for exact string matching
  - Transitions directly to accept states with corresponding boolean values
  - Returns Python `True` or `False` values

- JSON compliance:
  - Strictly case-sensitive (only "true"/"false", not "True"/"FALSE")
  - No whitespace allowed (unlike some other PSE state machines)
  - No partial matches or extra characters permitted

**Example: Boolean parsing**
```python
from pse.types.boolean import BooleanStateMachine

# Define a boolean state machine
bool_sm = BooleanStateMachine()

# This will match: "true" -> True (Python bool)
# This will match: "false" -> False (Python bool)
# This will not match: "True" (incorrect case)
# This will not match: " true" (has whitespace)
# This will not match: "truthy" (extra characters)
```

**Integration example:**
```python
from pse.types.boolean import BooleanStateMachine
from pse.types.json.json_object import ObjectSchemaStateMachine

# Using boolean state machine inside a JSON object
settings_sm = ObjectSchemaStateMachine(
    schema={
        "type": "object",
        "properties": {
            "enabled": {"type": "boolean"},
            "notifications": {"type": "boolean"}
        }
    },
    context={}
)

# This will match: {"enabled": true, "notifications": false}
```

## ArrayStateMachine

`ArrayStateMachine` handles JSON array structures, processing sequences of elements enclosed in square brackets.

```python
from pse.types.array import ArrayStateMachine

# Create a basic array state machine for JSON arrays
array_sm = ArrayStateMachine()
```

**Implementation details:**
- State machine structure:
  - State 0: Initial state that accepts opening bracket "["
  - State 1: Whitespace handling or immediate closing bracket for empty arrays
  - State 2: Value state that uses JsonStateMachine to parse elements
  - State 3: Whitespace handling after each value
  - State 4: Decision point (comma for next item or "]" to close array)

- Array validation:
  - Enforces proper structure with opening/closing brackets
  - Ensures comma-separated values with no trailing commas
  - Properly validates nested array structures
  - Supports whitespace between array elements and brackets

- Value collection:
  - Uses ArrayStepper to track parsed values
  - Maintains immutability by cloning steppers during processing
  - Returns a list of parsed JSON values

**ArraySchemaStateMachine extension**:
For JSON Schema integration and type-specific arrays, use ArraySchemaStateMachine:

```python
from pse.types.json.json_array import ArraySchemaStateMachine

# Create a typed array state machine for arrays of strings
string_array_sm = ArraySchemaStateMachine(
    schema={
        "type": "array",
        "items": {"type": "string"}
    },
    context={}
)

# Create an array with length constraints and unique items
constrained_array_sm = ArraySchemaStateMachine(
    schema={
        "type": "array",
        "items": {"type": "number"},
        "minItems": 1,
        "maxItems": 5,
        "uniqueItems": True
    },
    context={}
)
```

**Example: Basic array parsing**
```python
from pse.types.array import ArrayStateMachine

# Define an array state machine
array_sm = ArrayStateMachine()

# This will match: "[]" -> [] (empty array)
# This will match: "[1, 2, 3]" -> [1, 2, 3] (array of numbers)
# This will match: '["a", "b", "c"]' -> ["a", "b", "c"] (array of strings)
# This will match: '[true, false]' -> [True, False] (array of booleans)
```

**Example: Nested array parsing**
```python
from pse.types.array import ArrayStateMachine

# Define an array state machine
array_sm = ArrayStateMachine()

# This will match: "[[1, 2], [3, 4]]" -> [[1, 2], [3, 4]] (nested array)
# This will match: '[{"a": 1}, {"b": 2}]' -> [{"a": 1}, {"b": 2}] (array of objects)
```

**Example: Schema validation**
```python
from pse.types.json.json_array import ArraySchemaStateMachine

# Create a schema-enforced array of integers
integers_array_sm = ArraySchemaStateMachine(
    schema={
        "type": "array",
        "items": {"type": "integer"},
        "minItems": 2
    },
    context={}
)

# This will match: "[1, 2, 3]" -> [1, 2, 3]
# This will not match: "[1]" (too few items)
# This will not match: "[1.5, 2]" (non-integer value)
```

## ObjectStateMachine

`ObjectStateMachine` processes JSON object structures, handling key-value pairs enclosed in curly braces.

```python
from pse.types.object import ObjectStateMachine

# Create a basic object state machine
object_sm = ObjectStateMachine()

# Create an object state machine that can be optional (empty objects allowed)
optional_object_sm = ObjectStateMachine(is_optional=True)
```

**Implementation details:**
- State machine structure:
  - State 0: Initial state that accepts opening brace "{"
  - State 1: Whitespace handling after opening brace
  - State 2: Key-value pair processing using KeyValueStateMachine
  - State 3: Whitespace handling after key-value pair
  - State 4: Decision point (comma for next property or "}" to close object)

- Object validation:
  - Enforces proper structure with opening/closing braces
  - Ensures comma-separated properties with no trailing commas
  - Properly handles nested object structures
  - Supports whitespace between properties and braces

- Value collection:
  - Uses ObjectStepper to accumulate key-value pairs in a dictionary
  - Maintains immutability by cloning steppers during processing
  - Returns a dictionary of parsed property values

- Parameters:
  - `is_optional`: When True, allows empty objects (default: False)

**ObjectSchemaStateMachine extension**:
For JSON Schema integration and property validation, use ObjectSchemaStateMachine:

```python
from pse.types.json.json_object import ObjectSchemaStateMachine

# Create a schema-based object state machine
person_object_sm = ObjectSchemaStateMachine(
    schema={
        "type": "object",
        "properties": {
            "name": {"type": "string"},
            "age": {"type": "integer", "minimum": 0},
            "email": {"type": "string", "format": "email"}
        },
        "required": ["name", "email"]
    },
    context={}
)
```

**Example: Basic object parsing**
```python
from pse.types.object import ObjectStateMachine

# Define an object state machine
object_sm = ObjectStateMachine()

# This will match: "{}" -> {} (empty object, only if is_optional=True)
# This will match: '{"a": 1}' -> {"a": 1} (simple object)
# This will match: '{"name": "Alice", "age": 30}' -> {"name": "Alice", "age": 30} (multiple properties)
```

**Example: Nested object parsing**
```python
from pse.types.object import ObjectStateMachine

# Define an object state machine
object_sm = ObjectStateMachine()

# This will match complex nested objects:
# {
#   "user": {
#     "name": "Alice",
#     "address": {
#       "city": "New York",
#       "zip": "10001"
#     }
#   }
# }
```

**Example: Working with object values**
```python
from pse.types.object import ObjectStateMachine

object_sm = ObjectStateMachine()
input_str = '{"a": 1, "b": true, "c": "hello"}'

# Parse the object string
steppers = object_sm.get_steppers()
for char in input_str:
    steppers = object_sm.advance_all_steppers(steppers, char)

# Get the parsed object from the first valid stepper
if steppers:
    parsed_object = steppers[0].value  # {"a": 1, "b": True, "c": "hello"}

    # Access individual properties
    value_a = parsed_object["a"]  # 1
```

## KeyValueStateMachine

`KeyValueStateMachine` handles JSON key-value pairs within objects, processing the structure of `"key": value`.

```python
from pse.types.key_value import KeyValueStateMachine

# Create a basic key-value pair state machine
kv_sm = KeyValueStateMachine()

# Create an optional key-value pair state machine
optional_kv_sm = KeyValueStateMachine(is_optional=True)
```

**Implementation details:**
- State machine structure:
  - By default, uses a sequence of five state machines:
    1. StringStateMachine() for the key
    2. WhitespaceStateMachine() for whitespace after key
    3. PhraseStateMachine(":") for the colon separator
    4. WhitespaceStateMachine() for whitespace after colon
    5. JsonStateMachine() for the value
  - Transitions through each component in sequence
  - Returns a tuple of (key, value) upon successful parsing

- Parameters:
  - `sequence`: Optional customized sequence of state machines (default: as described above)
  - `is_optional`: Whether the key-value pair is optional (default: False)

- Key-value tracking:
  - Uses KeyValueStepper to track property name and value
  - Extracts key using json.loads() to handle quote removal and escaping
  - Value is processed by JsonStateMachine and can be any valid JSON type
  - Maintains immutability through proper stepper cloning

**Example: Basic key-value parsing**
```python
from pse.types.key_value import KeyValueStateMachine

# Define a key-value state machine
kv_sm = KeyValueStateMachine()

# This will match: '"name": "Alice"' -> ("name", "Alice")
# This will match: '"age": 30' -> ("age", 30)
# This will match: '"active": true' -> ("active", True)
# This will match: '"data": {"x": 1, "y": 2}' -> ("data", {"x": 1, "y": 2})
```

**Example: Using key-value pairs in objects**
```python
from pse.types.object import ObjectStateMachine
from pse.types.key_value import KeyValueStateMachine

# ObjectStateMachine uses KeyValueStateMachine internally
object_sm = ObjectStateMachine()

# This uses KeyValueStateMachine to parse each property
input_str = '{"name": "Alice", "age": 30}'

steppers = object_sm.get_steppers()
for char in input_str:
    steppers = object_sm.advance_all_steppers(steppers, char)

# Result is a dictionary built from key-value pairs
result = steppers[0].value  # {"name": "Alice", "age": 30}
```

**KeyValueSchemaStateMachine extension**:
For JSON Schema validation of property pairs, the schema-aware extension provides additional validation:

```python
from pse.types.json.json_key_value import KeyValueSchemaStateMachine

# Create a schema-validated key-value pair
email_property_sm = KeyValueSchemaStateMachine(
    property_name="email",
    property_schema={"type": "string", "format": "email"},
    context={},
    required=True
)

# This will match: '"email": "user@example.com"'
# This will not match: '"email": 123' (invalid type)
# This will not match: '"email": "invalid"' (invalid format)
```

## WhitespaceStateMachine

`WhitespaceStateMachine` handles whitespace characters (spaces, tabs, newlines, carriage returns) in structured formats.

```python
from pse.types.whitespace import WhitespaceStateMachine

# Create a whitespace state machine with default settings
# This makes whitespace optional (min=0) with a maximum of 20 characters
ws_sm = WhitespaceStateMachine()

# Create a whitespace state machine with specific constraints
required_ws_sm = WhitespaceStateMachine(
    min_whitespace=1,  # Require at least one whitespace character
    max_whitespace=10  # Allow at most 10 whitespace characters
)
```

**Implementation details:**
- State machine structure:
  - Extends CharacterStateMachine with whitespace characters (" \t\n\r")
  - Accepts consecutive whitespace characters up to max_whitespace
  - Becomes optional when min_whitespace is set to 0 (default)
  - Returns the matched whitespace characters as a string

- Parameters:
  - `min_whitespace`: Minimum required whitespace characters (default: 0)
  - `max_whitespace`: Maximum allowed whitespace characters (default: 20)
  - When `min_whitespace=0`, acts as an optional whitespace matcher

- Matching behavior:
  - Greedy matching: consumes as many whitespace characters as possible
  - Stops consuming when encountering non-whitespace characters
  - Successfully validates when at least min_whitespace characters have been consumed
  - Rejects input when more than max_whitespace characters are encountered

**Example: Basic whitespace handling**
```python
from pse.types.whitespace import WhitespaceStateMachine

# Define a whitespace state machine
ws_sm = WhitespaceStateMachine()

# This will match: "" (empty string, since min_whitespace=0)
# This will match: " " (single space)
# This will match: "\t\n  " (mixed whitespace)
# This will not match: "a" (non-whitespace character)
```

**Example: Required whitespace**
```python
from pse.types.whitespace import WhitespaceStateMachine

# Define a whitespace state machine requiring at least one character
required_ws_sm = WhitespaceStateMachine(min_whitespace=1)

# This will not match: "" (empty string)
# This will match: " " (single space)
# This will match: "\t\n  " (mixed whitespace)
```

**Example: Integration with other state machines**
```python
from pse.types.base.chain import ChainStateMachine
from pse.types.base.phrase import PhraseStateMachine
from pse.types.whitespace import WhitespaceStateMachine

# Create a state machine that parses "key = value" format
assignment_sm = ChainStateMachine([
    PhraseStateMachine("key"),         # Match "key"
    WhitespaceStateMachine(),          # Optional whitespace
    PhraseStateMachine("="),           # Match "="
    WhitespaceStateMachine(),          # Optional whitespace
    PhraseStateMachine("value")        # Match "value"
])

# This will match: "key=value"
# This will match: "key = value"
# This will match: "key  =  value"
```

**Common usage in PSE:**
- Handles optional whitespace between tokens in structured formats
- Creates flexible parsers that handle various formatting styles
- Enables human-readable output with proper spacing
- Used extensively in JSON, XML, and other format-specific state machines


---
pse-docs/docs/api/state-machines/formats.md
---
# Format-Specific State Machines

PSE includes specialized state machines for common data formats, making it easy to generate structured outputs that conform to standard specifications.

## JSON State Machines

PSE provides a suite of state machines specifically designed for working with JSON data.

### JsonStateMachine

`JsonStateMachine` serves as the entry point for parsing any valid JSON value. It functions as a dispatcher to specialized state machines for different JSON types.

```python
from pse.types.json.json_value import JsonStateMachine

# Create a general JSON state machine that can parse any valid JSON value
json_sm = JsonStateMachine()
```

**Implementation details:**
- Implements a simple two-state machine:
  - State 0: Initial state with multiple outgoing edges to specialized parsers
  - All parsers transition to a single final state ("$") upon completion
- Delegates parsing to specialized state machines for each JSON type:
  - `ObjectStateMachine` for JSON objects
  - `ArrayStateMachine` for JSON arrays
  - `StringStateMachine` for JSON strings
  - `PhraseStateMachine` for "null" literal
  - `BooleanStateMachine` for true/false
  - `NumberStateMachine` for numeric values
- Uses default steppers from each specialized parser
- Provides valid continuations for any valid JSON value

**Usage in the PSE hierarchy:**
- Acts as the root node for all JSON type parsing
- Used as the default value type in JSON arrays when no schema is specified
- Primary entry point for generic JSON parsing without schema validation
- Can be used directly or as part of more complex state machines

### ObjectSchemaStateMachine

`ObjectSchemaStateMachine` implements schema-aware JSON object parsing and generation, extending the base `ObjectStateMachine` to handle JSON Schema validation.

```python
from pse.types.json.json_object import ObjectSchemaStateMachine

# Define a schema for a person object
person_schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "age": {"type": "integer", "minimum": 0},
        "hobbies": {"type": "array", "items": {"type": "string"}}
    },
    "required": ["name", "hobbies"]
}

# Create a schema-based object state machine
person_object_sm = ObjectSchemaStateMachine(person_schema, {})
```

**Implementation details:**
- Parameters:
  - `schema`: JSON Schema dictionary for object validation
  - `context`: Context information for parsing
  - `properties`: Schema properties (default: {})
  - `required_property_names`: List of required properties (default: [])
  - `additional_properties`: Controls whether extra properties are allowed (default: {})
  - `ordered_properties`: Whether properties must be in order (default: True)

- State graph structure:
  - State 0: Accepts "{" to start object
  - State 1: Whitespace handling after opening brace
  - State 2: Property parsing with schema validation
  - State 3: Whitespace after property
  - State 4: Decision point (comma for next property or "}" to end)

- Schema processing:
  - Automatically processes `nullable` and `default` values to determine truly required properties
  - Properties with default values aren't considered required for parsing
  - Special handling for pattern properties and additional properties

- Validation mechanisms:
  - Only allows object closure when all required properties are present
  - Enforces property ordering when `ordered_properties` is true
  - Controls which properties can be included based on schema definitions

**Additional property handling:**
- When `additional_properties` is true (default): Allows any properties beyond the defined ones
- When `additional_properties` is false: Rejects properties not defined in the schema
- When `additional_properties` is a schema object: Validates extra properties against that schema

**Example with property ordering and additional properties:**
```python
from pse.types.json.json_object import ObjectSchemaStateMachine

# Schema with ordered properties and restricted additional properties
contact_schema = {
    "type": "object",
    "properties": {
        "id": {"type": "string"},
        "name": {"type": "string"},
        "email": {"type": "string", "format": "email"}
    },
    "required": ["id", "name"],
    "additionalProperties": {
        "type": "string"  # Additional props must be strings
    },
    "orderedProperties": True  # Must follow schema order
}

contact_sm = ObjectSchemaStateMachine(
    schema=contact_schema,
    context={},
    ordered_properties=True  # Can also be set via constructor
)
```

**Nested schema example:**
```python
# Schema with nested objects
nested_schema = {
    "type": "object",
    "properties": {
        "user": {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "address": {
                    "type": "object",
                    "properties": {
                        "street": {"type": "string"},
                        "city": {"type": "string"}
                    },
                    "required": ["street"]
                }
            },
            "required": ["name"]
        }
    },
    "required": ["user"]
}

# Create nested schema state machine
nested_sm = ObjectSchemaStateMachine(nested_schema, {})
```

### ArraySchemaStateMachine

`ArraySchemaStateMachine` implements schema-aware JSON array processing, extending the base `ArrayStateMachine` to handle JSON Schema validation.

```python
from pse.types.json.json_array import ArraySchemaStateMachine

# Create a JSON array state machine for arrays of strings
string_array_sm = ArraySchemaStateMachine(
    schema={"type": "array", "items": {"type": "string"}},
    context={}
)

# Create a JSON array with length constraints
bounded_array_sm = ArraySchemaStateMachine(
    schema={"type": "array", "items": {"type": "number"}, "minItems": 1, "maxItems": 5},
    context={}
)
```

**Implementation details:**
- Parameters:
  - `schema`: JSON Schema dictionary for array validation
  - `context`: Context for schema resolution containing refs and path information

- State graph structure:
  - State 0: Initial state accepting '[' to start the array
  - State 1: Whitespace handling after opening bracket or decision if empty array is valid
  - State 2: Item parsing using schema from 'items' property
  - State 3: Whitespace handling after each item
  - State 4: Decision point (comma for next item or ']' to close)

- Custom stepper implementation:
  - `ArraySchemaStepper` extends `ArrayStepper` with schema validation
  - Tracks item history for uniqueItems enforcement
  - Enforces minItems/maxItems constraints during parsing and generation
  - Preserves item order when filtering duplicates for uniqueItems

- Schema processing:
  - Extracts constraints from schema including minItems, maxItems, and uniqueItems
  - Validates items against 'items' schema property
  - Handles prefixItems and contains constraints
  - Supports tuple validation with different schemas per position

**uniqueItems behavior:**
When `uniqueItems` is true, duplicates are filtered while preserving the order of first occurrence:
```python
# Array with unique items constraint
unique_array_sm = ArraySchemaStateMachine(
    schema={
        "type": "array",
        "items": {"type": "number"},
        "uniqueItems": True
    },
    context={}
)

# This would only allow [1, 2, 3], not [1, 2, 1, 3]
```

**Advanced constraints example:**
```python
# Array with multiple constraints
complex_array_sm = ArraySchemaStateMachine(
    schema={
        "type": "array",
        "items": {
            "type": "object",
            "properties": {
                "id": {"type": "string"},
                "value": {"type": "number"}
            },
            "required": ["id"]
        },
        "minItems": 1,
        "maxItems": 10,
        "uniqueItems": True
    },
    context={"path": "root"}
)
```

**Integration with other state machines:**
```python
# Nested structure with arrays inside objects
nested_schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "tags": {
            "type": "array",
            "items": {"type": "string"},
            "uniqueItems": True
        },
        "scores": {
            "type": "array",
            "items": {"type": "number"},
            "minItems": 3
        }
    },
    "required": ["name", "scores"]
}

# The object state machine will automatically use ArraySchemaStateMachine
# for the array properties
nested_sm = ObjectSchemaStateMachine(nested_schema, {})
```

### AnySchemaStateMachine

`AnySchemaStateMachine` provides a powerful way to create state machines from complex JSON Schemas with logical composition operators.

```python
from pse.types.json.any_json_schema import AnySchemaStateMachine

complex_schemas = [
    {"type": "string"},
    {"type": "number"},
    {
        "type": "object",
        "properties": {
            "status": {"type": "string", "enum": ["success", "error"]},
            "data": {"type": "object"}
        },
        "required": ["status"]
    }
]

# Create a state machine for the complex schema
complex_sm = AnySchemaStateMachine(
    schemas=complex_schemas,
    context={}
)
```

**Implementation details:**
- Parameters:
  - `schemas`: List of JSON Schema objects to validate against
  - `context`: Context for schema resolution containing refs and path information

- State machine structure:
  - Creates individual state machines for each schema
  - Constructs transitions from the initial state to each schema state machine
  - Accepts if any schema path reaches an accept state

- Parallel path exploration:
  - Maintains multiple steppers to validate against different schemas simultaneously
  - Returns the first successfully validated value
  - Only fails if all schema paths fail to validate
  - Preserves the correctly typed value for the matching schema

**JSON Schema logical composition:**
`AnySchemaStateMachine` is used internally for handling JSON Schema's logical composition operators:

```python
# These schema patterns are automatically processed using AnySchemaStateMachine
schema1 = {
    "oneOf": [  # Must match exactly one schema
        {"type": "string"},
        {"type": "number"}
    ]
}

schema2 = {
    "anyOf": [  # Can match any number of schemas
        {"type": "string"},
        {"type": "number"}
    ]
}

schema3 = {
    "type": ["string", "number"]  # Union type (string OR number)
}

schema4 = {
    "type": "string",
    "nullable": True  # String OR null
}
```

**Usage for complex schema validation:**
```python
# Create a state machine for a payment method schema
payment_method_sm = AnySchemaStateMachine(
    schemas=[
        {  # Credit card schema
            "type": "object",
            "properties": {
                "type": {"type": "string", "enum": ["credit_card"]},
                "card_number": {"type": "string", "pattern": "^[0-9]{16}$"},
                "expiration": {"type": "string", "pattern": "^[0-9]{2}/[0-9]{2}$"},
                "cvv": {"type": "string", "pattern": "^[0-9]{3,4}$"}
            },
            "required": ["type", "card_number", "expiration", "cvv"]
        },
        {  # Bank transfer schema
            "type": "object",
            "properties": {
                "type": {"type": "string", "enum": ["bank_transfer"]},
                "account_number": {"type": "string"},
                "routing_number": {"type": "string"}
            },
            "required": ["type", "account_number", "routing_number"]
        },
        {  # Digital wallet schema
            "type": "object",
            "properties": {
                "type": {"type": "string", "enum": ["digital_wallet"]},
                "provider": {"type": "string", "enum": ["paypal", "venmo", "apple_pay"]},
                "email": {"type": "string", "format": "email"}
            },
            "required": ["type", "provider", "email"]
        }
    ],
    context={"path": "payment_method"}
)
```

**Performance considerations:**
- Shares the token processing pipeline across all paths to minimize overhead
- Designed for efficient exploration of multiple potential schema matches
- Automatically prunes invalid paths as tokens are processed

## XML State Machines

PSE includes state machines for working with XML data.

### XMLTagStateMachine

`XMLTagStateMachine` parses XML opening and closing tags with a specific tag name.

```python
from pse.types.xml.xml_tag import XMLTagStateMachine

# Create an XML opening tag state machine with a specific tag name
person_tag_sm = XMLTagStateMachine(tag_name="person")

# Create an XML closing tag state machine
person_closing_tag_sm = XMLTagStateMachine(tag_name="person", closing_tag=True)
```

**Implementation details:**
- Parameters:
  - `tag_name`: The name of the XML tag to match (required)
  - `closing_tag`: Boolean flag to indicate if it's a closing tag (default: False)

- Internal structure:
  - Inherits from `ChainStateMachine`
  - Creates a chain of `PhraseStateMachine` instances that match parts of the tag:
    1. Opening delimiter: `<` for normal tags, `</` for closing tags
    2. The tag name itself
    3. Closing delimiter: `>`

- Custom stepper behavior:
  - Implements `XMLTagStepper` that extends `ChainStepper`
  - Overrides `get_valid_continuations()` to return the complete tag as a single valid continuation
  - Ensures proper tag name matching with exact case sensitivity

**Usage notes:**
- Currently focused on basic tag recognition without attributes
- Does not support self-closing tags (e.g., `<br/>`)
- Whitespace is not allowed within tags
- Used as a building block for `XMLEncapsulatedStateMachine`

**Simple example:**
```python
from pse.types.xml.xml_tag import XMLTagStateMachine

# Match a specific opening tag
h1_tag_sm = XMLTagStateMachine(tag_name="h1")

# This will match "<h1>" but not "<h2>" or "<H1>"
```

### XMLEncapsulatedStateMachine

`XMLEncapsulatedStateMachine` handles complete XML elements with content between matching tags.

```python
from pse.types.xml.xml_encapsulated import XMLEncapsulatedStateMachine
from pse.types.base.phrase import PhraseStateMachine

# Create an XML element with text content
text_element_sm = XMLEncapsulatedStateMachine(
    tag_name="p",  # XML tag name
    state_machine=PhraseStateMachine("Hello world"),  # Content state machine
    min_buffer_length=-1,  # Minimum buffer length (default: -1)
    is_optional=False  # Whether the element is optional (default: False)
)
```

**Implementation details:**
- Parameters:
  - `tag_name`: Name of the XML tag (required)
  - `state_machine`: The inner state machine that processes content between tags (required)
  - `min_buffer_length`: Minimum buffer length before attempting to match (default: -1)
  - `is_optional`: Whether the element is optional (default: False)

- State graph structure:
  - State 0: Uses `WaitFor` with `XMLTagStateMachine` to find opening tag
  - State 1: Processes content using the provided inner state machine
  - State 2: Looks for closing tag using `XMLTagStateMachine` with closing_tag=True

- Custom stepper behavior:
  - Uses `EncapsulatedStepper` for tracking buffer and inner state
  - Maintains proper nesting by requiring matching opening and closing tags
  - Returns validated content from between the tags

**Complex content example:**
```python
from pse.types.xml.xml_encapsulated import XMLEncapsulatedStateMachine
from pse.types.json.json_value import JsonStateMachine

# XML element containing JSON content
json_in_xml_sm = XMLEncapsulatedStateMachine(
    tag_name="data",
    state_machine=JsonStateMachine()
)

# This will match: <data>{"key": "value"}</data>
```

**Nested XML elements:**
```python
from pse.types.xml.xml_encapsulated import XMLEncapsulatedStateMachine
from pse.types.base.chain import ChainStateMachine
from pse.types.base.phrase import PhraseStateMachine

# Create a nested XML structure for HTML-like content
paragraph_sm = XMLEncapsulatedStateMachine(
    tag_name="p",
    state_machine=PhraseStateMachine("Sample text")
)

# A div containing a paragraph
div_with_paragraph_sm = XMLEncapsulatedStateMachine(
    tag_name="div",
    state_machine=paragraph_sm
)

# This will match: <div><p>Sample text</p></div>
```

**Optional XML element:**
```python
from pse.types.xml.xml_encapsulated import XMLEncapsulatedStateMachine
from pse.types.base.phrase import PhraseStateMachine

# Optional XML element
optional_element_sm = XMLEncapsulatedStateMachine(
    tag_name="optional",
    state_machine=PhraseStateMachine("Some content"),
    is_optional=True
)

# Will match both "<optional>Some content</optional>" and empty string
```

## Grammar State Machines

PSE provides state machines for parsing according to formal grammars.

Currently, this approach is kind of experimental and not all grammars are supported.

Manual massaging of the lark grammars is required to get the best results.

We've provided a few pre-configured grammars - for valid python and bash syntax - but again these are hacks and not a full solution.

In the future we want to natively support BNF grammars and have a more robust solution.

**Integration with other state machines:**
```python
from pse.types.base.encapsulated import EncapsulatedStateMachine
from pse.types.grammar.lark import LarkGrammarStateMachine
from pse.types.grammar.default_grammars.python import PythonGrammar

# Create a state machine for code blocks containing Python code
python_code_block = EncapsulatedStateMachine(
    left_delimiter="```python\n",
    right_delimiter="\n```",
    state_machine=LarkGrammarStateMachine(PythonGrammar())
)
```

### PythonStateMachine

`PythonStateMachine` is a pre-configured instance of `LarkGrammarStateMachine` that specializes in parsing valid Python code.

```python
from pse.types.grammar.default_grammars.python import PythonStateMachine

# Use the pre-configured Python state machine
python_sm = PythonStateMachine

# Use with encapsulation for markdown-style code blocks
from pse.types.base.encapsulated import EncapsulatedStateMachine
python_block_sm = EncapsulatedStateMachine(
    state_machine=PythonStateMachine,
    left_delimiter="```python\n",
    right_delimiter="\n```"
)
```

**Implementation details:**
- Based on `LarkGrammarStateMachine` using a specialized `PythonGrammar` class
- Uses a comprehensive Lark grammar file (`python.lark`) that covers Python 3.x syntax
- Employs a custom Python indentation handler for proper handling of indentation-sensitive code
- Default delimiters for markdown-style code blocks: `"```python\n"` and `"\n```"`
- Implements special validation logic for Python syntax

**Python-specific validation features:**
- Intelligent handling of incomplete code during generation
- Special processing for indent/dedent tokens
- Proper handling of unclosed quotes, brackets, and parentheses
- Support for all Python syntax including:
  - Function and class definitions
  - Control flow statements
  - List/dict/set comprehensions
  - Async/await syntax
  - Type annotations
  - Decorators and lambda expressions

**Example: Python code validation**
```python
from pse.types.grammar.default_grammars.python import PythonStateMachine

# Parse a simple Python function
code = """
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n-1)
"""

# Get steppers and advance through the code
steppers = PythonStateMachine.get_steppers()
for char in code:
    steppers = PythonStateMachine.advance_all_steppers(steppers, char)

# Check if any stepper successfully parsed the code
valid = any(s.has_reached_accept_state() for s in steppers)
```

**Example: Integration with structuring engine**
```python
from pse import StructuringEngine
from pse.types.grammar.default_grammars.python import PythonStateMachine
from pse.types.base.encapsulated import EncapsulatedStateMachine

# Create a Python code block parser
python_block = EncapsulatedStateMachine(
    state_machine=PythonStateMachine,
    left_delimiter="```python\n",
    right_delimiter="\n```"
)

# Create structuring engine with the Python block state machine
engine = StructuringEngine().configure(python_block)

# Generate code with LLM
response = engine.generate("Write a function to calculate the Fibonacci sequence", model="claude-3-sonnet-20240229")
```

**Example: Partial code validation during generation**
```python
from pse.types.grammar.default_grammars.python import PythonStateMachine

# Even incomplete code is accepted during non-strict validation
incomplete_code = "def factorial(n):\n    if n <= 1:"
steppers = PythonStateMachine.get_steppers()
steppers = PythonStateMachine.advance_all_steppers(steppers, incomplete_code)

# Steppers remain valid for incomplete but syntactically correct code
assert len(steppers) > 0
```

### BashStateMachine

`BashStateMachine` is a pre-configured instance of `LarkGrammarStateMachine` that specializes in parsing and generating Bash shell code.

```python
from pse.types.grammar.default_grammars.bash import BashStateMachine

# Use the pre-configured Bash state machine
bash_sm = BashStateMachine

# Use with encapsulation for markdown-style code blocks
from pse.types.base.encapsulated import EncapsulatedStateMachine
bash_block_sm = EncapsulatedStateMachine(
    state_machine=BashStateMachine,
    left_delimiter="```bash\n",
    right_delimiter="\n```"
)
```

**Implementation details:**
- Based on `LarkGrammarStateMachine` using a specialized `BashGrammar` class
- Uses a comprehensive Lark grammar file (`bash.lark`) that covers Bash shell syntax
- Default delimiters for markdown-style code blocks: `"```bash\n"` and `"\n```"`
- Implements special validation logic for Bash-specific syntax structures

**Bash-specific validation features:**
- Intelligent handling of incomplete commands during generation
- Special processing for unclosed quotes and control structures
- Support for all Bash syntax including:
  - Control flow (if/then/else, for/while/until loops, case statements)
  - Variables and parameter expansion (`$var`, `${var}`)
  - Command substitution (`$(command)`) and subshells
  - I/O redirections (`>`, `>>`, `<`, `2>&1`, etc.)
  - Function definitions
  - Arithmetic expressions (`$(( expr ))`)
  - Comments and shebangs

**Example: Bash script validation**
```python
from pse.types.grammar.default_grammars.bash import BashStateMachine

# Parse a simple Bash script
script = """
#!/bin/bash

function greet() {
    local name=$1
    echo "Hello, $name!"
}

if [ $# -eq 0 ]; then
    echo "Usage: $0 <name>"
    exit 1
fi

greet $1
"""

# Check if the script is valid
is_valid = BashStateMachine.grammar.validate(script, strict=True)
```

**Example: Integration with structuring engine**
```python
from pse import StructuringEngine
from pse.types.grammar.default_grammars.bash import BashStateMachine
from pse.types.base.encapsulated import EncapsulatedStateMachine

# Create a Bash code block parser
bash_block = EncapsulatedStateMachine(
    state_machine=BashStateMachine,
    left_delimiter="```bash\n",
    right_delimiter="\n```"
)

# Create structuring engine with the Bash block state machine
engine = StructuringEngine().configure(bash_block)

# Generate shell script with LLM
response = engine.generate(
    "Write a Bash script to find the largest files in a directory",
    model="claude-3-sonnet-20240229"
)
```

**Example: Handling incomplete Bash code**
```python
from pse.types.grammar.default_grammars.bash import BashStateMachine

# Incomplete code that's syntactically valid so far
incomplete_code = "if [ -f /etc/passwd ]; then\n  grep 'root' /etc/passwd"

# In non-strict mode, this should be accepted
is_valid_partial = BashStateMachine.grammar.validate(incomplete_code, strict=False)
assert is_valid_partial  # True

# In strict mode, this would be rejected
is_valid_strict = BashStateMachine.grammar.validate(incomplete_code, strict=True)
assert not is_valid_strict  # False
```

**Performance considerations:**
- Parses Bash syntax incrementally, providing good performance
- Uses efficient path pruning to avoid exploring invalid syntax paths
- Handles complex scripts including nested structures
- Token healing enables recovery from minor syntax errors

## Mixed-Content State Machines

PSE includes state machines for handling mixed content formats.

### FencedFreeformStateMachine

`FencedFreeformStateMachine` allows free-form text between specific delimiters, functioning as an enhanced version of `EncapsulatedStateMachine` that accepts any character content.

```python
from pse.types.misc.fenced_freeform import FencedFreeformStateMachine

# Create a state machine that captures content between markdown-style fences
python_block_sm = FencedFreeformStateMachine(
    identifier="python",  # Specifies fence type (becomes ```python\n and \n```)
)

# Create a state machine with custom delimiters
custom_block_sm = FencedFreeformStateMachine(
    delimiters=("<START>", "<END>"),  # Custom opening and closing delimiters
    char_min=10,  # Minimum content length
    char_max=1000,  # Maximum content length
    is_optional=False  # Whether the content is required
)
```

**Implementation details:**
- Parameters:
  - `identifier`: Optional language identifier (e.g., "python" produces ```python\n and \n``` fences)
  - `delimiters`: Tuple of opening and closing delimiters (overrides identifier-based fences)
  - `char_min`: Minimum number of characters required (default: 0)
  - `char_max`: Maximum number of characters allowed (default: None)
  - `buffer_length`: Controls character buffer size during processing (default: 32768)
  - `is_optional`: Whether the content is optional (default: False)

- Internal structure:
  - Extends `EncapsulatedStateMachine` with freeform character acceptance
  - Uses `CharacterStateMachine` internally with a whitelist containing all characters
  - Maintains character counting to enforce min/max constraints
  - Provides a `FencedFreeformStepper` for controlling character processing

- Fence behavior:
  - With identifier: Uses markdown-style code fences (```<identifier>\n and \n```)
  - With delimiters: Uses the provided custom delimiters
  - Both can be set to customize fence appearance

**Usage examples:**

**Example: Character limits**
```python
from pse.types.misc.fenced_freeform import FencedFreeformStateMachine

# Create a state machine for short descriptions (10-50 characters)
description_sm = FencedFreeformStateMachine(
    delimiters=("<desc>", "</desc>"),
    char_min=10,
    char_max=50
)

# This enforces both minimum and maximum length constraints
```

**Common applications:**
- Markdown-style code blocks in documentation
- Processing custom template languages
- Extracting specific sections from text
- Allowing structured and unstructured content in the same document
- Creating custom delimited content regions


---
pse-docs/docs/api/state-machines/schema-sources.md
---
# Schema Sources

PSE provides several ways to create state machines from schema definitions, enabling flexible integration with existing code and data models.

## JSON Schema

The most common way to define structures in PSE is using JSON Schema, a powerful standard for describing JSON data structures.

```python
from pse import StructuringEngine

# Define a JSON Schema
person_schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "age": {"type": "integer", "minimum": 0},
        "address": {
            "type": "object",
            "properties": {
                "street": {"type": "string"},
                "city": {"type": "string"},
                "zipcode": {"type": "string"}
            },
            "required": ["street", "city"]
        },
        "hobbies": {
            "type": "array",
            "items": {"type": "string"}
        }
    },
    "required": ["name", "age"]
}

# Create a structuring engine from the schema
engine = StructuringEngine.configure(person_schema)
```

**Key features:**
- Support for the full JSON Schema specification
- Nested object and array structures
- Validation constraints (min/max, required fields, patterns)
- Type validation and coercion

## Pydantic Models

PSE can create state machines directly from Pydantic models, integrating seamlessly with Python type definitions.

```python
from pydantic import BaseModel, Field
from typing import List, Optional
from pse import StructuringEngine

# Define a Pydantic model
class Address(BaseModel):
    street: str
    city: str
    zipcode: Optional[str] = None

class Person(BaseModel):
    name: str
    age: int = Field(ge=0)
    address: Optional[Address] = None
    hobbies: List[str] = []

# Create a structuring engine from the Pydantic model
engine = StructuringEngine.configure(Person)
```

**Key features:**
- Direct integration with Python type annotations
- Automatic schema generation from class definitions
- Support for nested models and complex types
- Validation using Pydantic's powerful validators

## Function Signatures

PSE can extract schema information from function signatures, enabling generation of valid function calls.

```python
from pse import StructuringEngine

def search_database(
    query: str,
    max_results: int = 10,
    filters: dict[str, str] | None = None,
    sort_by: list[str] = ["relevance"]
) -> list[dict]:
    """
    Search the database with the given parameters.

    Args:
        query: The search query string
        max_results: Maximum number of results to return
        filters: Optional dictionary of field/value filters
        sort_by: Fields to sort results by
    """
    pass

# Create a structuring engine from the function signature
engine = StructuringEngine.configure(search_database)
```

**Key features:**
- Extracts parameter types from function signatures
- Supports default values and optional parameters
- Handles complex nested types with type annotations
- Enables generation of valid function calls with proper arguments


---
pse-docs/docs/core-concepts/core-architecture.md
---
# Core Architecture

The Proxy Structuring Engine (PSE) achieves its structural guarantees through a layered architecture that tightly integrates grammar definition, state tracking, and language model interaction at runtime.

```mermaid
graph LR
    A[User Schema Definition <br/>(Pydantic, JSON Schema, Custom SM)] --> B(StructuringEngine <br/> Python Interface);
    C[LLM Backend <br/> (e.g., Transformers)] --> B;
    D[Tokenizer] --> B;

    subgraph PSE Core Logic
        direction TB
        B -- Configures --> E[StateMachine <br/> (Hierarchical State Graph)];
        B -- Manages --> F[Steppers <br/> (Active State Navigators)];
        E -- Defines Valid Transitions --> F;
        F -- Queries Valid Tokens --> B;
        B -- Modifies Logits --> C;
        C -- Samples Token --> B;
        B -- Advances Steppers --> F;
        F -- Tracks History --> G[Output Reconstruction];
    end

    B -- Retrieves Final Output --> G;
    G --> H[Guaranteed Structured Output <br/> (Python Object/String)];

    style A fill:#DAD0AF,stroke:#024645,color:#024645
    style H fill:#DAD0AF,stroke:#024645,color:#024645
    style C fill:#024645,stroke:#DAD0AF,color:#DAD0AF
    style D fill:#024645,stroke:#DAD0AF,color:#DAD0AF
    style B fill:#0c5460,stroke:#DAD0AF,color:#DAD0AF
    style E fill:#02323A,stroke:#DAD0AF,color:#DAD0AF
    style F fill:#02323A,stroke:#DAD0AF,color:#DAD0AF
    style G fill:#0c5460,stroke:#DAD0AF,color:#DAD0AF
```

## Key Components

1.  **User Schema Definition:** Developers define the desired output structure using familiar Python tools like Pydantic models, standard JSON Schema, Python function signatures, or by composing PSE's base `StateMachine` types (`Chain`, `Loop`, etc.).
2.  **`StructuringEngine` (Python Interface):** This is the main entry point. It takes the user's schema definition and the LLM's tokenizer. It translates the schema into the internal `StateMachine` representation and manages the interaction with the LLM during generation via the `process_logits` and `sample` hooks.
3.  **`StateMachine` (Core Concept):** Represents the defined grammar as a hierarchical state graph. Each node is a state, and edges are transitions validated by potentially nested `StateMachine`s. This graph defines all valid sequences of tokens according to the structure.
4.  **`Stepper` (Core Concept):** Represents an active position or hypothesis within the `StateMachine` graph. The `StructuringEngine` maintains one or more active `Stepper`s to track all possible valid paths through the grammar simultaneously. Steppers use the `StateMachine` to determine valid next moves.
5.  **Tokenizer:** Provided by the user (typically from the LLM framework), used by the `StructuringEngine` to map between strings and token IDs for vocabulary lookups, token healing, and multi-token processing.
6.  **LLM Backend:** The underlying language model (e.g., running via `transformers`). PSE integrates by intercepting the logits before sampling.
7.  **Output Reconstruction:** After generation finishes (a `Stepper` reaches an end state), the `StructuringEngine` uses the `Stepper`'s history and tracked token IDs to reconstruct the final output string accurately and parse it into the desired Python object.

## Runtime Enforcement Loop

The core guarantee comes from the tight loop during generation:

1.  LLM generates logits for the next token.
2.  `StructuringEngine.process_logits` queries all active `Stepper`s for valid next tokens based on their current state in the `StateMachine`.
3.  `process_logits` masks the logits, setting the probability of invalid tokens (those violating the grammar) to negative infinity.
4.  `StructuringEngine.sample` takes the masked logits and uses a base sampler.
5.  The sampled (guaranteed valid) token ID is used to advance the active `Stepper`s to their next state(s) via `Stepper.consume`.
6.  This loop repeats until an end state is reached.

This runtime enforcement ensures that every generated token strictly adheres to the structure defined by the `StateMachine`, eliminating the possibility of schema violations. Features like Token Healing and Multi-Token Processing add robustness and efficiency to this core loop.

---
pse-docs/docs/core-concepts/engine.md
---
# Engine

The `StructuringEngine` (Python class `pse.StructuringEngine`, inheriting from the core `Engine`) is the central orchestrator within PSE. It acts as the primary interface for developers, managing the interaction between the language model, the defined structure (`StateMachine`), and the tokenization process.

## Key Responsibilities

*   **Configuration:** Accepts high-level schema definitions (Pydantic models, JSON Schema, function signatures, or custom `StateMachine` objects) via its `configure` method. It translates these definitions into the internal `StateMachine` representation used for enforcement.
*   **LLM Integration:** Provides the necessary hooks (`process_logits`, `sample`) to integrate with language model generation loops (e.g., Hugging Face `transformers`).
    *   `process_logits`: Modifies the LLM's output logit distribution at each step, masking tokens that would violate the structure defined by the current state(s) of the active `Stepper`(s). This is the core of the structural guarantee.
    *   `sample`: Wraps the base sampling logic. It receives the processed logits, calls the base sampler, and then uses the sampled token ID(s) to advance the internal `Stepper`(s) according to the `StateMachine`. It handles multi-token processing and path selection (`StepperDelta`) internally.
*   **State Management:** Maintains the set of active `Stepper` objects, representing all valid hypotheses about the structure being generated according to the `StateMachine`.
*   **Tokenization Awareness:** Uses the provided tokenizer (e.g., from `transformers`) to map between token IDs and strings, enabling features like Token Healing and Multi-Token Processing by understanding the vocabulary.
*   **Output Retrieval:** Provides methods (`get_structured_output`, `get_stateful_structured_output`) to retrieve the final generated string (reconstructed accurately using token history) and parse/validate it against the target schema (e.g., into a Pydantic model).

## Role in PSE

The `StructuringEngine` ties all the components of PSE together:

1.  It takes the user-defined **Structure** (via `configure`).
2.  It uses the **Tokenizer** to understand the LLM's vocabulary.
3.  It manages the **Steppers** navigating the **StateMachine**.
4.  It modifies the **LLM's Output** (logits) at runtime.
5.  It handles **Sampling** and advances the state.
6.  It provides the final, **Guaranteed Structured Output**.

Developers primarily interact with the `StructuringEngine` instance to set up PSE and retrieve the results, while the underlying `StateMachine` and `Stepper` objects manage the detailed state traversal and validation internally.

---
pse-docs/docs/core-concepts/multi-token-processing.md
---
# Multi-Token Processing

Multi-Token Processing (MTP) is an optional optimization within the Proxy Structuring Engine (PSE) designed to accelerate generation when the grammar requires a specific sequence of characters that corresponds to multiple tokens in the language model's vocabulary.

## The Problem: Inefficient Generation for Fixed Sequences

Grammars often require specific keywords, operators, or phrases (e.g., `"function"`, `">="`, `"null"`). While PSE guarantees these are generated correctly, standard one-token-at-a-time generation can be inefficient if the sequence is long and represented by many tokens (e.g., `"metacognition"` might be tokenized as `["meta", "cognition"]`). If the grammar dictates that `"metacognition"` is the *only* valid continuation at a certain point, making the LLM generate `"meta"` and then separately generate `"cognition"` involves unnecessary steps.

## How Multi-Token Processing Optimizes

When MTP is enabled (`multi_token_sampling=True` in the `StructuringEngine`, default is `False`), PSE attempts to identify these unambiguous multi-token sequences:

1.  **Identify Required Sequence:** PSE determines the valid continuation strings required by the current state(s) of the grammar (e.g., only `"metacognition"` is allowed next).
2.  **Tokenize Sequence:** It finds the corresponding sequence of tokens from the model's vocabulary (e.g., `["meta", "cognition"]`).
3.  **Check for Ambiguity:** PSE checks if the *first* token of this sequence (e.g., `"meta"`) could *also* be the start of any *other* valid multi-token sequence allowed by the grammar at this exact point.
4.  **Unambiguous Case - Proactive Return:** If the sequence is unambiguous (i.e., only `"metacognition"` can follow the sampling of `"meta"` according to the grammar), PSE can proactively return the *entire* sequence of tokens (`["meta", "cognition"]`) to the generation loop in a single step, even though the LLM only sampled the first one.
5.  **Ambiguous Case - Cautious Return:** If the first token is ambiguous (e.g., the grammar allows both `"less_than"` represented by `["less", "_than"]` and `"less_than_or_equal_to"` represented by `["less", "_than", "_or", "_equal", "_to"]`), PSE will only return the common prefix sequence (`["less", "_than"]`) or potentially just the first sampled token (`"less"`). This avoids prematurely committing to the wrong grammatical path.

## Benefits

*   **Increased Efficiency:** Reduces the number of LLM forward passes required when the grammar dictates specific, unambiguous multi-token keywords or phrases.
*   **Faster Generation:** Can significantly speed up the generation of boilerplate text, fixed syntax elements, or required keywords within the structured output.

## User Experience

This optimization works automatically behind the scenes when enabled. Users define their grammar using strings and high-level constructs; PSE handles the token-level analysis and potential multi-token returns to improve generation speed where possible without sacrificing the structural guarantee.

---
pse-docs/docs/core-concepts/state-graph-traversal.md
---
# State Graph Traversal

The core of the Proxy Structuring Engine's (PSE) reliability and efficiency lies in its unique approach to navigating the grammar defined by the `StateMachine`. Unlike traditional parsing methods that might rely on backtracking, PSE employs a parallel, forward-only traversal algorithm that explores all valid structural possibilities simultaneously.

## Core Principles

PSE's traversal algorithm is built on these key principles:

1.  **Hierarchical State Machines (HSMs):** Grammars are represented as graphs where transitions between states are themselves governed by nested `StateMachine`s. This allows for representing complex, recursive structures efficiently.
2.  **Active Steppers:** The engine maintains a list of active `Stepper` objects. Each `Stepper` represents a current hypothesis or valid position within the `StateMachine` graph.
3.  **Parallel Exploration:** When a token is processed, *all* active `Stepper`s attempt to consume it. A single token might lead to multiple valid next states, resulting in the creation of new `Stepper` instances. This explores all valid paths concurrently.
4.  **Immutability:** `Stepper` operations (like consuming a token) typically return *new* stepper instances representing the next state, rather than modifying the original. This prevents state interference between parallel paths.
5.  **Deterministic Path Selection:** When multiple valid paths exist after processing a token (represented by multiple `StepperDelta` objects), a deterministic algorithm (`StepperDelta.choose_best_path`) selects the single "best" path based on criteria like reaching an accept state, token healing status, and token probability scores. This resolves ambiguity without backtracking.

## The Traversal Loop (Simplified)

During generation, the process at each step looks roughly like this:

1.  **Get Valid Continuations:** The `StructuringEngine` queries all active `Stepper`s using `get_valid_continuations()` to determine the set of all possible next token strings allowed by the grammar from the current positions.
2.  **Mask Logits:** The engine uses this information to mask the LLM's raw logits, setting the probability of invalid tokens to negative infinity (`process_logits`).
3.  **Sample Token:** The engine calls the base sampler on the masked logits. The sampled token ID is guaranteed to be structurally valid (`sample`).
4.  **Advance Steppers:** The engine uses the sampled token ID(s) to advance the set of active `Stepper`s. This involves:
    *   Decoding the token ID(s) to string(s).
    *   Calling `StateMachine.advance_all` (which internally uses `Stepper.consume`) on the active steppers with the token string. This generates a list of `StepperDelta` objects, representing potential next states.
    *   Applying `StepperDelta.choose_best_path` to select the single best token path and its associated resulting `Stepper`(s).
    *   Updating the list of active `Stepper`s for the next generation step.
5.  **Repeat:** The loop continues until a `Stepper` reaches a designated `end_state` in the root `StateMachine`.

This forward-only, parallel exploration combined with deterministic path selection allows PSE to handle complex and ambiguous grammars efficiently while guaranteeing structural correctness at every step.

---
pse-docs/docs/core-concepts/state-machine.md
---
# State Machine

The `StateMachine` is the core component in PSE responsible for defining the structure or grammar that the language model's output must adhere to. It represents this structure as a graph where states are nodes and transitions between states are edges.

## Key Concepts

*   **Graph Structure:** A `StateMachine` defines a directed graph. Each node represents a specific point or state within the grammar (e.g., "expecting an opening brace," "inside a string," "expecting a comma or closing bracket").
*   **States (`StateId`):** States are identified by either an integer or a string. Every state machine has a designated `start_state` and one or more `end_states` which signify a valid completion of the structure defined by that machine.
*   **Transitions (`Edge`):** An edge represents a valid transition from one state to another. Crucially, each transition is associated with *another* `StateMachine` instance. This nested `StateMachine` must be successfully traversed (i.e., reach its end state) for the transition in the parent machine to be considered valid. This allows for hierarchical and recursive grammar definitions.
*   **Hierarchy:** Because transitions are themselves `StateMachine`s, complex grammars (like nested JSON objects or code structures) can be built by composing smaller, simpler `StateMachine`s.
*   **Traversal:** The actual process of moving through the state machine during generation is handled by `Stepper` objects (see [Stepper Concept](./stepper.md)). The `StateMachine` provides methods like `get_transitions` and `advance_stepper` that the `Stepper` uses to determine valid moves based on incoming tokens.
*   **Properties:** State machines can have properties like `is_optional` (allowing the entire structure they define to be skipped) and `is_case_sensitive` (controlling token matching).

## Role in PSE

When you configure the `StructuringEngine` with a schema (like a Pydantic model or JSON Schema), PSE translates that schema into a root `StateMachine` object. This object, potentially composed of many nested `StateMachine`s, represents the complete grammar. The engine then uses this `StateMachine` graph at runtime to guide the LLM's token generation, ensuring the output strictly conforms to the defined structure. You can also build custom `StateMachine`s directly using PSE's base types for more complex control flows.

---
pse-docs/docs/core-concepts/stepper.md
---
# Stepper

If the `StateMachine` defines the *state map*, the `Stepper` is the *navigator* that tracks the current position within that map during generation. It represents an active state or hypothesis about the structure being generated.

## Key Concepts

*   **Position Tracking:** A `Stepper` holds the `current_state` within its associated `StateMachine`. It knows where it is in the state graph.
*   **Immutability:** Steppers generally follow an immutable pattern. Operations like consuming a token (`consume`) or branching (`branch`) typically return *new* `Stepper` instances representing the subsequent state(s), rather than modifying the original stepper. This is crucial for exploring multiple valid paths simultaneously in non-deterministic grammars.
*   **Hierarchical Traversal:** When a `StateMachine` transition involves a nested `StateMachine`, the parent `Stepper` creates and manages a `sub_stepper` to traverse the nested machine. The parent stepper only advances its own state (`current_state` -> `target_state`) once the `sub_stepper` successfully reaches an end state.
*   **History:** A `Stepper` maintains a `history` list, which stores the sequence of completed sub-steppers that led to its current state. This allows reconstruction of the generated structure.
*   **Value Accumulation:** Steppers accumulate the raw string output (`_raw_value`) corresponding to the path they've taken. Methods like `get_current_value` can parse this raw string into appropriate Python types (though final parsing is usually handled by the `StructuringEngine`).
*   **State Checks:** Steppers provide methods to query their status:
    *   `has_reached_accept_state()`: Checks if the stepper (and its sub-stepper, if active) is currently in a valid end state according to its `StateMachine`.
    *   `can_accept_more_input()`: Checks if further tokens can be consumed based on grammar rules (e.g., character limits).
    *   `is_within_value()`: Indicates if the stepper is actively consuming characters for a specific value (e.g., inside a string literal).
    *   `get_valid_continuations()`: Returns the set of strings that are valid next steps from the current state.
*   **Interaction with StateMachine:** Steppers use their associated `StateMachine` to determine valid transitions (`get_transitions`) and to advance their state (`advance_stepper`).

## Role in PSE

During generation, the `StructuringEngine` maintains a list of active `Stepper` objects. When processing logits, it queries these steppers (via `get_valid_continuations`) to determine which tokens are allowed next. When sampling a token, the engine uses the steppers' `consume` method to generate the next set of active steppers, reflecting the updated positions in the grammar graph. The engine manages potentially multiple active steppers simultaneously to handle grammatical ambiguity, eventually selecting the best path based on criteria like reaching an accept state and token probabilities (see `StepperDelta`).

---
pse-docs/docs/core-concepts/token-healing.md
---
# Token Healing

Token Healing is a robustness feature within the Proxy Structuring Engine (PSE) designed to handle minor discrepancies between the exact token generated by the Language Model (LLM) and the canonical token expected by the grammar (StateMachine). It allows PSE to recover gracefully when the generated token contains slight variations (like extra spaces or minor artifacts) but still clearly corresponds to a valid structural continuation.

## The Problem: Tokenization Artifacts

LLMs operate on tokens, and sometimes the tokenization process introduces subtle variations compared to the clean strings defined in a grammar.

*Example:*
*   A `StateMachine` expects the exact string `"example"` as the next valid token.
*   The vocabulary contains a canonical token for `"example"` (e.g., ID 456).
*   However, due to tokenization quirks, the LLM might generate a slightly different token, like ID 123, which decodes to `" example "` (with leading/trailing spaces).

Without Token Healing, when the engine receives `" example "`, the `StateMachine` might only consume the `"example"` part, leaving the spaces as `remaining_input`. This could potentially cause the generation path to be incorrectly penalized or discarded, even though the LLM likely intended to produce the correct structural element. Standard multi-token sequences (like generating `"exam"` then `"ple"` to form `"example"`) are handled naturally by the Stepper's recursive consumption and are *not* the target of Token Healing.

## How Token Healing Works

Token Healing activates specifically when a call to `stepper->consume(token)` results in a stepper state where `remaining_input` is *not* null. This indicates the provided `token` was longer than what the current `StateMachine` step could fully process.

1.  **Partial Consumption:** The `StateMachine` consumes the longest valid prefix it can from the input `token`. The unconsumed part is left as `remaining_input`.
2.  **Prefix Identification:** The portion of the input `token` that *was* successfully consumed is identified (let's call this `consumed_prefix`).
3.  **Vocabulary Lookup:** PSE performs an efficient lookup in the vocabulary (the HAT-trie provided to the `Engine`) using `vocab->longest_prefix(consumed_prefix)`. This search finds the longest token string stored in the vocabulary that *starts with* `consumed_prefix`.
4.  **Canonical Match Found:** If the lookup finds such a token in the vocabulary (let's call it `canonical_token`), PSE assumes this `canonical_token` was the LLM's true intention.
5.  **Healing Applied:**
    *   A new `StepperDelta` is created, associating the transition with the `canonical_token` string found in the vocabulary (not the original, slightly deviant input `token`).
    *   This `StepperDelta` is marked with `was_healed = true`.
    *   The `remaining_input` on the resulting `Stepper` is cleared, as the healing process assumes the full `canonical_token` was intended and processed.
6.  **Path Selection:** When choosing the best path forward (`StepperDelta::choose_best_path`), the engine prioritizes paths that did *not* require healing (`was_healed = false`). Healing serves as a fallback to recover potentially valid paths that would otherwise be discarded due to minor token deviations.

## Benefits

*   **Resilience to Tokenization Noise:** Makes generation robust against common tokenization artifacts like inconsistent spacing around words or punctuation.
*   **Reduced False Negatives:** Prevents valid generation paths from being prematurely abandoned due to minor, recoverable token mismatches.
*   **Improved Reliability:** Increases the overall success rate of generating complex, guaranteed structures, especially with models or tokenizers prone to minor inconsistencies.

Token Healing enhances PSE's practical reliability by bridging the gap between the precise requirements of the `StateMachine` and the sometimes noisy reality of token-based LLM generation. It operates automatically when a vocabulary is provided to the engine.

---
